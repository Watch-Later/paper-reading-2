---
title: Contractive Auto-Encoders - Explicit Invariance During Feature Extraction
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Contractive Auto-Encoders - Explicit Invariance During Feature Extraction

## References

- [Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion](./stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion.md)
- Training with Noise is Equivalent to Tikhonov Regularization
- Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
- Sparse coding with an overcomplete basis set - A strategy employed by V1?
- Neural networks and principal component analysis - Learning from examples without local minima
- [Learning Multiple Layers of Features from Tiny Images](./learning-multiple-layers-of-features-from-tiny-images.md)
- [A Fast Learning Algorithm for Deep Belief Nets](./a-fast-learning-algorithm-for-deep-belief-nets.md)
- Learning invariant features through topographic filter maps
- [Greedy Layer-Wise Training of Deep Networks](./greedy-layer-wise-training-of-deep-networks.md)
- Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
- What is the best multi-stage architecture for object recognition?
- Sparse deep belief net model for visual area V2
- Deep learning via semi-supervised embedding
- Nonlinear Autoassociation Is Not Equivalent to PCA
- Learning representations by backpropagating errors
- Learning representations by back-propagating errors
- [Learning Deep Architectures for AI](./learning-deep-architectures-for-ai.md)
- An empirical evaluation of deep architectures on problems with many factors of variation
- Learning representations by back-propagation errors, nature
- Advances in Neural Information Processing Systems 8 (NIPS 1995)
