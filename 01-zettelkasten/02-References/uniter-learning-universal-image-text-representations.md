---
title: UNITER - Learning UNiversal Image-TExt Representations
authors:
- Yen-Chun Chen
- Linjie Li
- Licheng Yu
- Ahmed El Kholy
- Faisal Ahmed
- Zhe Gan
- Yu Cheng
- Jingjing Liu
fieldsOfStudy:
- Computer Science
meta_key: uniter-learning-universal-image-text-representations
numCitedBy: 282
pdf_relpath: null
ref_count: 57
status: todo
tags:
- gen-from-ref
- paper
venue: ECCV 2020
year: 2019
---

# UNITER - Learning UNiversal Image-TExt Representations

## References

- [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](./unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training.md)
- [Learning Deep Structure-Preserving Image-Text Embeddings](./learning-deep-structure-preserving-image-text-embeddings.md)
- [Stacked Cross Attention for Image-Text Matching](./stacked-cross-attention-for-image-text-matching.md)
- [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](./vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.md)
- [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](./multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding.md)
- [VideoBERT - A Joint Model for Video and Language Representation Learning](./videobert-a-joint-model-for-video-and-language-representation-learning.md)
- [MAttNet - Modular Attention Network for Referring Expression Comprehension](./mattnet-modular-attention-network-for-referring-expression-comprehension.md)
- [Context Encoders - Feature Learning by Inpainting](./context-encoders-feature-learning-by-inpainting.md)
- [Deep Visual-Semantic Alignments for Generating Image Descriptions](./deep-visual-semantic-alignments-for-generating-image-descriptions.md)
- [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](./lxmert-learning-cross-modality-encoder-representations-from-transformers.md)
- [12-in-1 - Multi-Task Vision and Language Representation Learning](./12-in-1-multi-task-vision-and-language-representation-learning.md)
- [Deep Modular Co-Attention Networks for Visual Question Answering](./deep-modular-co-attention-networks-for-visual-question-answering.md)
- [VisualBERT - A Simple and Performant Baseline for Vision and Language](./visualbert-a-simple-and-performant-baseline-for-vision-and-language.md)
- Visual Entailment - A Novel Task for Fine-Grained Image Understanding
- [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](./vl-bert-pre-training-of-generic-visual-linguistic-representations.md)
- [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](./conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning.md)
- [Unsupervised Visual Representation Learning by Context Prediction](./unsupervised-visual-representation-learning-by-context-prediction.md)
- [Unsupervised Representation Learning by Predicting Image Rotations](./unsupervised-representation-learning-by-predicting-image-rotations.md)
- Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](./bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.md)
- [Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](./unsupervised-learning-of-visual-representations-by-solving-jigsaw-puzzles.md)
- [Fusion of Detected Objects in Text for Visual Question Answering](./fusion-of-detected-objects-in-text-for-visual-question-answering.md)
- [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](./albert-a-lite-bert-for-self-supervised-learning-of-language-representations.md)
- [Bilinear Attention Networks](./bilinear-attention-networks.md)
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](./visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations.md)
- [From Recognition to Cognition - Visual Commonsense Reasoning](./from-recognition-to-cognition-visual-commonsense-reasoning.md)
- [Colorful Image Colorization](./colorful-image-colorization.md)
- [Learning Video Representations using Contrastive Bidirectional Transformer](./learning-video-representations-using-contrastive-bidirectional-transformer.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- [Learning Generative Models with Sinkhorn Divergences](./learning-generative-models-with-sinkhorn-divergences.md)
- [Modeling Context in Referring Expressions](./modeling-context-in-referring-expressions.md)
- NLVR2 Visual Bias Analysis
- [GQA - A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](./gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering.md)
- [Microsoft COCO - Common Objects in Context](./microsoft-coco-common-objects-in-context.md)
- [Scaling Neural Machine Translation](./scaling-neural-machine-translation.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- Im2Text - Describing Images Using 1 Million Captioned Photographs
- [Distilling the Knowledge in a Neural Network](./distilling-the-knowledge-in-a-neural-network.md)
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- [A Corpus for Reasoning about Natural Language Grounded in Photographs](./a-corpus-for-reasoning-about-natural-language-grounded-in-photographs.md)
- [Revealing the Dark Secrets of BERT](./revealing-the-dark-secrets-of-bert.md)
- ReferItGame - Referring to Objects in Photographs of Natural Scenes
- [Decoupled Weight Decay Regularization](./decoupled-weight-decay-regularization.md)
- [Improving GANs Using Optimal Transport](./improving-gans-using-optimal-transport.md)
- [Automatic differentiation in PyTorch](./automatic-differentiation-in-pytorch.md)
- A Fast Proximal Point Method for Wasserstein Distance
- [Wasserstein Generative Adversarial Networks](./wasserstein-generative-adversarial-networks.md)
- A Fast Proximal Point Method for Computing Wasserstein Distance
- [Sinkhorn Distances - Lightspeed Computation of Optimal Transport](./sinkhorn-distances-lightspeed-computation-of-optimal-transport.md)
- [Computational Optimal Transport](./computational-optimal-transport.md)
- Selfie - Self-supervised Pretraining for Image Embedding
- VQA - Visual Question Answering
