---
title: Oscar - Object-Semantics Aligned Pre-training for Vision-Language Tasks
authors:
- Xiujun Li
- Xi Yin
- Chunyuan Li
- Xiaowei Hu
- Pengchuan Zhang
- Lei Zhang
- Lijuan Wang
- Houdong Hu
- Li Dong
- Furu Wei
- Yejin Choi
- Jianfeng Gao
fieldsOfStudy:
- Computer Science
meta_key: oscar-object-semantics-aligned-pre-training-for-vision-language-tasks
numCitedBy: 534
pdf_relpath: null
ref_count: 52
status: todo
tags:
- gen-from-ref
- paper
venue: ECCV
year: 2020
---

# Oscar - Object-Semantics Aligned Pre-training for Vision-Language Tasks

## References

- [Unified Vision-Language Pre-Training for Image Captioning and VQA](./unified-vision-language-pre-training-for-image-captioning-and-vqa.md)
- [UNITER - UNiversal Image-TExt Representation Learning](./uniter-universal-image-text-representation-learning.md)
- [ImageBERT - Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data](./imagebert-cross-modal-pre-training-with-large-scale-weak-supervised-image-text-data.md)
- [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](./unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training.md)
- [UNITER - Learning UNiversal Image-TExt Representations](./uniter-learning-universal-image-text-representations.md)
- [DeViSE - A Deep Visual-Semantic Embedding Model](./devise-a-deep-visual-semantic-embedding-model.md)
- [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](./vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.md)
- [Stacked Cross Attention for Image-Text Matching](./stacked-cross-attention-for-image-text-matching.md)
- Connecting modalities - Semi-supervised segmentation and annotation of images using unaligned text corpora
- [The Open Images Dataset V4](./the-open-images-dataset-v4.md)
- [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](./unifying-visual-semantic-embeddings-with-multimodal-neural-language-models.md)
- [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](./lxmert-learning-cross-modality-encoder-representations-from-transformers.md)
- Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training
- [Dual-path Convolutional Image-Text Embeddings with Instance Loss](./dual-path-convolutional-image-text-embeddings-with-instance-loss.md)
- [Image Captioning with Semantic Attention](./image-captioning-with-semantic-attention.md)
- [CAMP - Cross-Modal Adaptive Message Passing for Text-Image Retrieval](./camp-cross-modal-adaptive-message-passing-for-text-image-retrieval.md)
- Zero-Shot Learning by Convex Combination of Semantic Embeddings
- [VideoBERT - A Joint Model for Video and Language Representation Learning](./videobert-a-joint-model-for-video-and-language-representation-learning.md)
- [What Value Do Explicit High Level Concepts Have in Vision to Language Problems?](./what-value-do-explicit-high-level-concepts-have-in-vision-to-language-problems.md)
- [The Open Images Dataset V4](./the-open-images-dataset-v4.md)
- Joint Image-Text Representation by Gaussian Visual-Semantic Embedding
- [VisualBERT - A Simple and Performant Baseline for Vision and Language](./visualbert-a-simple-and-performant-baseline-for-vision-and-language.md)
- [Zero-Shot Learning Through Cross-Modal Transfer](./zero-shot-learning-through-cross-modal-transfer.md)
- Knowledge Aware Semantic Concept Expansion for Image-Text Matching
- [Guided Open Vocabulary Image Captioning with Constrained Beam Search](./guided-open-vocabulary-image-captioning-with-constrained-beam-search.md)
- Position Focused Attention Network for Image-Text Matching
- [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](./visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations.md)
- [Multi-Task Deep Neural Networks for Natural Language Understanding](./multi-task-deep-neural-networks-for-natural-language-understanding.md)
- [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](./bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.md)
- [The Open Images Dataset V4](./the-open-images-dataset-v4.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- nocaps - novel object captioning at scale
- [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](./making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering.md)
- [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](./conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning.md)
- [VSE++ - Improving Visual-Semantic Embeddings with Hard Negatives](./vse-improving-visual-semantic-embeddings-with-hard-negatives.md)
- [Microsoft COCO - Common Objects in Context](./microsoft-coco-common-objects-in-context.md)
- [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](./faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.md)
- [Attention on Attention for Image Captioning](./attention-on-attention-for-image-captioning.md)
- [The Open Images Dataset V4](./the-open-images-dataset-v4.md)
- [A Corpus for Reasoning about Natural Language Grounded in Photographs](./a-corpus-for-reasoning-about-natural-language-grounded-in-photographs.md)
- [Learning by Abstraction - The Neural State Machine](./learning-by-abstraction-the-neural-state-machine.md)
- [Self-Critical Sequence Training for Image Captioning](./self-critical-sequence-training-for-image-captioning.md)
- Meta Module Network for Compositional Visual Reasoning
- Im2Text - Describing Images Using 1 Million Captioned Photographs
- Aligning Sentences in Parallel Corpora
- [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](./from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions.md)
- [Visualizing Data using t-SNE](./visualizing-data-using-t-sne.md)
- [The Open Images Dataset V4](./the-open-images-dataset-v4.md)
