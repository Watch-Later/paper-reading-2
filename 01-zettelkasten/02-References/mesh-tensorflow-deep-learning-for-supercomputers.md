---
title: Mesh-TensorFlow - Deep Learning for Supercomputers
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Mesh-TensorFlow - Deep Learning for Supercomputers

## References

- Integrated Model, Batch, and Domain Parallelism in Training Neural Networks
- Beyond Data and Model Parallelism for Deep Neural Networks
- Integrated Model and Data Parallelism in Training Neural Networks
- Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks
- A massively parallel tensor contraction framework for coupled-cluster computations
- [Large Scale Distributed Deep Networks](./large-scale-distributed-deep-networks.md)
- [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](./outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer.md)
- Scalable task-based algorithm for multiplication of block-rank-sparse matrices
- Communication-Avoiding Optimization Methods for Distributed Massive-Scale Sparse Inverse Covariance Estimation
- [ImageNet classification with deep convolutional neural networks](./imagenet-classification-with-deep-convolutional-neural-networks.md)
- Minimizing Communication in Numerical Linear Algebra
- [Attention is All you Need](./attention-is-all-you-need.md)
- Optimal bucket algorithms for large MPI collectives on torus interconnects
- Spatially Parallel Convolutions
- Communication-Optimal Convolutional Neural Nets
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- Communication-Optimal Parallel Recursive Rectangular Matrix Multiplication
- Communication-Avoiding Parallel Sparse-Dense Matrix-Matrix Multiplication
- Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms
- Communication lower bounds and optimal algorithms for programs that reference arrays - Part 1
- Communication lower bounds for distributed-memory matrix multiplication
- More iteration space tiling
- Communication optimal parallel multiplication of sparse random matrices
- Bandwidth optimal all-reduce algorithms for clusters of workstations
- Communication efficient matrix multiplication on hypercubes
- Communication Complexity of PRAMs
