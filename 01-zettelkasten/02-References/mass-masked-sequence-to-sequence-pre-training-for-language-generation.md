---
title: MASS - Masked Sequence to Sequence Pre-training for Language Generation
authors:
- Kaitao Song
- Xu Tan
- Tao Qin
- Jianfeng Lu
- Tie-Yan Liu
fieldsOfStudy:
- Computer Science
meta_key: mass-masked-sequence-to-sequence-pre-training-for-language-generation
numCitedBy: 599
pdf_relpath: null
ref_count: 59
status: todo
tags:
- gen-from-ref
- paper
venue: ICML
year: 2019
---

# MASS - Masked Sequence to Sequence Pre-training for Language Generation

## References

- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- Transfer Learning for Low-Resource Neural Machine Translation
- [Unsupervised Pretraining for Sequence to Sequence Learning](./unsupervised-pretraining-for-sequence-to-sequence-learning.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- MaskGAN - Better Text Generation via Filling in the ______
- [Attention is All you Need](./attention-is-all-you-need.md)
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- Unsupervised Neural Machine Translation with Weight Sharing
- Phrase-Based & Neural Unsupervised Machine Translation
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- [Skip-Thought Vectors](./skip-thought-vectors.md)
- Exploiting Source-side Monolingual Data in Neural Machine Translation
- [Universal Language Model Fine-tuning for Text Classification](./universal-language-model-fine-tuning-for-text-classification.md)
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- Neural Headline Generation with Minimum Risk Training
- [Neural Machine Translation of Rare Words with Subword Units](./neural-machine-translation-of-rare-words-with-subword-units.md)
- Unsupervised Machine Translation Using Monolingual Corpora Only
- UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY
- Unsupervised Neural Machine Translation
- Double Path Networks for Sequence to Sequence Learning
- Semi-supervised Sequence Learning
- Multilingual Neural Machine Translation with Knowledge Distillation
- Unsupervised Pivot Translation for Distant Languages
- Almost Unsupervised Text to Speech and Automatic Speech Recognition
- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- Unsupervised Statistical Machine Translation
- Zero-Resource Translation with Multi-Lingual Neural Machine Translation
- [Cross-lingual Language Model Pretraining](./cross-lingual-language-model-pretraining.md)
- Machine Comprehension by Text-to-Text Neural Question Generation
- [Convolutional Sequence to Sequence Learning](./convolutional-sequence-to-sequence-learning.md)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- Neural Responding Machine for Short-Text Conversation
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- Dense Information Flow for Neural Machine Translation
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- Domain Adaptation with Structural Correspondence Learning
- Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization
- Achieving Human Parity on Automatic Chinese to English News Translation
- [Distributed Representations of Sentences and Documents](./distributed-representations-of-sentences-and-documents.md)
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- [An efficient framework for learning sentence representations](./an-efficient-framework-for-learning-sentence-representations.md)
- A Neural Probabilistic Language Model
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](./rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.md)
- A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data
- Chameleons in Imagined Conversations - A New Approach to Understanding Coordination of Linguistic Style in Dialogs
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Extracting and composing robust features with denoising autoencoders](./extracting-and-composing-robust-features-with-denoising-autoencoders.md)
- [Going deeper with convolutions](./going-deeper-with-convolutions.md)
- Learning Deep Representation with Large-Scale Attributes
- Class-Based n-gram Models of Natural Language
- Introduction to the CoNLL-2003 Shared Task - Language-Independent Named Entity Recognition
