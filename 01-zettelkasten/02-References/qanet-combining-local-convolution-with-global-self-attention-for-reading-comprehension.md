---
title: QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension

## References

- [Attention is All you Need](./attention-is-all-you-need.md)
- Multi-Perspective Context Matching for Machine Comprehension
- [Effective Approaches to Attention-based Neural Machine Translation](./effective-approaches-to-attention-based-neural-machine-translation.md)
- Learning Recurrent Span Representations for Extractive Question Answering
- DiSAN - Directional Self-Attention Network for RNN/CNN-free Language Understanding
- [Gated Self-Matching Networks for Reading Comprehension and Question Answering](./gated-self-matching-networks-for-reading-comprehension-and-question-answering.md)
- Globally Normalized Reader
- [Machine Comprehension Using Match-LSTM and Answer Pointer](./machine-comprehension-using-match-lstm-and-answer-pointer.md)
- MEMEN - Multi-layer Embedding with Memory Networks for Machine Comprehension
- Learning to Skim Text
- Attention-over-Attention Neural Networks for Reading Comprehension
- [Bidirectional Attention Flow for Machine Comprehension](./bidirectional-attention-flow-for-machine-comprehension.md)
- Improving Neural Machine Translation Models with Monolingual Data
- [TriviaQA - A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](./triviaqa-a-large-scale-distantly-supervised-challenge-dataset-for-reading-comprehension.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering
- Making Neural QA as Simple as Possible but not Simpler
- Stochastic Answer Networks for Machine Reading Comprehension
- ReasoNet - Learning to Stop Reading in Machine Comprehension
- [Simple and Effective Multi-Paragraph Reading Comprehension](./simple-and-effective-multi-paragraph-reading-comprehension.md)
- Reading Wikipedia to Answer Open-Domain Questions
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Dynamic Coattention Networks For Question Answering](./dynamic-coattention-networks-for-question-answering.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext
- [WikiReading - A Novel Large-scale Language Understanding Task over Wikipedia](./wikireading-a-novel-large-scale-language-understanding-task-over-wikipedia.md)
- Learning to Paraphrase for Question Answering
- [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](./the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations.md)
- Neural Question Generation from Text - A Preliminary Study
- Ruminating Reader - Reasoning with Gated Multi-hop Attention
- [Convolutional Sequence to Sequence Learning](./convolutional-sequence-to-sequence-learning.md)
- Structural Embedding of Syntactic Trees for Machine Comprehension
- Depthwise Separable Convolutions for Neural Machine Translation
- [Long Short-Term Memory](./long-short-term-memory.md)
- [Adversarial Examples for Evaluating Reading Comprehension Systems](./adversarial-examples-for-evaluating-reading-comprehension-systems.md)
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](./empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
- [Deep Networks with Stochastic Depth](./deep-networks-with-stochastic-depth.md)
- [Xception - Deep Learning with Depthwise Separable Convolutions](./xception-deep-learning-with-depthwise-separable-convolutions.md)
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- [Convolutional Neural Networks for Sentence Classification](./convolutional-neural-networks-for-sentence-classification.md)
- [Layer Normalization](./layer-normalization.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- Paraphrasing Revisited with Neural Machine Translation
- [TensorFlow - Large-Scale Machine Learning on Heterogeneous Distributed Systems](./tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems.md)
- [Character-level Convolutional Networks for Text Classification](./character-level-convolutional-networks-for-text-classification.md)
- [Highway Networks](./highway-networks.md)
- Reinforced Mnemonic Reader for Machine Comprehension
- End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking
