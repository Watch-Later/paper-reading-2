---
title: Understanding the difficulty of training deep feedforward neural networks
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Understanding the difficulty of training deep feedforward neural networks

## References

- Exploring Strategies for Training Deep Neural Networks
- [Greedy Layer-Wise Training of Deep Networks](./greedy-layer-wise-training-of-deep-networks.md)
- The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training
- [A Fast Learning Algorithm for Deep Belief Nets](./a-fast-learning-algorithm-for-deep-belief-nets.md)
- [Learning Multiple Layers of Features from Tiny Images](./learning-multiple-layers-of-features-from-tiny-images.md)
- [Learning Deep Architectures for AI](./learning-deep-architectures-for-ai.md)
- [Extracting and composing robust features with denoising autoencoders](./extracting-and-composing-robust-features-with-denoising-autoencoders.md)
- An empirical evaluation of deep architectures on problems with many factors of variation
- Learning long-term dependencies with gradient descent is difficult
- Learning representations by back-propagating errors
- Efficient Learning of Sparse Representations with an Energy-Based Model
- [Gradient-based learning applied to document recognition](./gradient-based-learning-applied-to-document-recognition.md)
- Accelerated Learning in Layered Neural Networks
- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- A Scalable Hierarchical Distributed Language Model
- Unsupervised Learning of Probabilistic Grammar-Markov Models for Object Categories
- Deep learning via semi-supervised embedding
- Learning in modular systems
- [Efficient BackProp](./efficient-backprop.md)
