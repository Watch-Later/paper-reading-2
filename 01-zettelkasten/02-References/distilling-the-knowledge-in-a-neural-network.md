---
title: Distilling the Knowledge in a Neural Network
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Distilling the Knowledge in a Neural Network

## References

- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- Learning small-size DNN with output-distribution-based criteria
- [ImageNet classification with deep convolutional neural networks](./imagenet-classification-with-deep-convolutional-neural-networks.md)
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- [Large Scale Distributed Deep Networks](./large-scale-distributed-deep-networks.md)
- Model compression
- Adaptive Mixtures of Local Experts
- [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](./deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups.md)
- Ensemble Methods in Machine Learning
