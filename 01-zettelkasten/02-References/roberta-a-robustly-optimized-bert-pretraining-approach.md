---
title: RoBERTa - A Robustly Optimized BERT Pretraining Approach
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# RoBERTa - A Robustly Optimized BERT Pretraining Approach

## References

- [SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems](./superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.md)
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- Cloze-driven Pretraining of Self-attention Networks
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- Reducing BERT Pre-Training Time from 3 Days to 76 Minutes
- [SpanBERT - Improving Pre-training by Representing and Predicting Spans](./spanbert-improving-pre-training-by-representing-and-predicting-spans.md)
- [Automatic differentiation in PyTorch](./automatic-differentiation-in-pytorch.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Scaling Neural Machine Translation](./scaling-neural-machine-translation.md)
- A Surprisingly Robust Trick for the Winograd Schema Challenge
- [Universal Language Model Fine-tuning for Text Classification](./universal-language-model-fine-tuning-for-text-classification.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- Fine-tuned Language Models for Text Classification
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- [A Simple Method for Commonsense Reasoning](./a-simple-method-for-commonsense-reasoning.md)
- [Multi-Task Deep Neural Networks for Natural Language Understanding](./multi-task-deep-neural-networks-for-natural-language-understanding.md)
- Semi-supervised Sequence Learning
- [Cross-lingual Language Model Pretraining](./cross-lingual-language-model-pretraining.md)
- Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding
- Mixed Precision Training
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](./unified-language-model-pre-training-for-natural-language-understanding-and-generation.md)
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- [MASS - Masked Sequence to Sequence Pre-training for Language Generation](./mass-masked-sequence-to-sequence-pre-training-for-language-generation.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- KERMIT - Generative Insertion-Based Modeling for Sequences
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- fairseq - A Fast, Extensible Toolkit for Sequence Modeling
- [Know What You Don't Know - Unanswerable Questions for SQuAD](./know-what-you-don-t-know-unanswerable-questions-for-squad.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- [RACE - Large-scale ReAding Comprehension Dataset From Examinations](./race-large-scale-reading-comprehension-dataset-from-examinations.md)
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](./a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference.md)
- The Sixth PASCAL Recognizing Textual Entailment Challenge
- [Neural Network Acceptability Judgments](./neural-network-acceptability-judgments.md)
- ERNIE - Enhanced Representation through Knowledge Integration
- [Neural Machine Translation of Rare Words with Subword Units](./neural-machine-translation-of-rare-words-with-subword-units.md)
- The Second PASCAL Recognising Textual Entailment Challenge
- Defending Against Neural Fake News
- The Seventh PASCAL Recognizing Textual Entailment Challenge
- [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](./aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books.md)
- The PASCAL Recognising Textual Entailment Challenge
- [Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units](./bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units.md)
- The Winograd Schema Challenge
- [Gaussian Error Linear Units (GELUs)](./gaussian-error-linear-units-gelus.md)
- Automatically Constructing a Corpus of Sentential Paraphrases
- news-please - A Generic News Crawler and Extractor
- The Third PASCAL Recognizing Textual Entailment Challenge
