---
title: Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations

## References

- Regularizing RNNs by Stabilizing Activations
- Surprisal-Driven Zoneout
- [Recurrent Batch Normalization](./recurrent-batch-normalization.md)
- Learning to Forget - Continual Prediction with LSTM
- Swapout - Learning an ensemble of deep architectures
- RNNDROP - A novel dropout for RNNS in ASR
- Recurrent Dropout without Memory Loss
- A Clockwork RNN
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
- Dropout Improves Recurrent Neural Networks for Handwriting Recognition
- Learning with Pseudo-Ensembles
- Learning Longer Memory in Recurrent Neural Networks
- [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](./a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
- [Recurrent Neural Network Regularization](./recurrent-neural-network-regularization.md)
- On Fast Dropout and its Applicability to Recurrent Networks
- How to Construct Deep Recurrent Neural Networks
- Understanding the exploding gradient problem
- Fast dropout training
- [Long Short-Term Memory](./long-short-term-memory.md)
- Describing Videos by Exploiting Temporal Structure
- Hierarchical Recurrent Neural Networks for Long-Term Dependencies
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- Learning long-term dependencies with gradient descent is difficult
- [Hierarchical Multiscale Recurrent Neural Networks](./hierarchical-multiscale-recurrent-neural-networks.md)
- [Deep Networks with Stochastic Depth](./deep-networks-with-stochastic-depth.md)
- [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation](./estimating-or-propagating-gradients-through-stochastic-neurons-for-conditional-computation.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- EESEN - End-to-end speech recognition using deep RNN models and WFST-based decoding
- [Character-Aware Neural Language Models](./character-aware-neural-language-models.md)
- [Layer Normalization](./layer-normalization.md)
- BinaryConnect - Training Deep Neural Networks with binary weights during propagations
- Blocks and Fuel - Frameworks for deep learning
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](./under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
- [Theano - A Python framework for fast computation of mathematical expressions](./theano-a-python-framework-for-fast-computation-of-mathematical-expressions.md)
- Building a Large Annotated Corpus of English - The Penn Treebank
- Untersuchungen zu dynamischen neuronalen Netzen
- Test Data
