---
title: Recurrent Neural Network Grammars
authors:
- Chris Dyer
- A. Kuncoro
- Miguel Ballesteros
- Noah A. Smith
fieldsOfStudy:
- Computer Science
meta_key: recurrent-neural-network-grammars
numCitedBy: 424
pdf_relpath: null
ref_count: 63
status: todo
tags:
- gen-from-ref
- paper
venue: NAACL
year: 2016
---

# Recurrent Neural Network Grammars

## References

- Generative Incremental Dependency Parsing with Neural Networks
- Easy-First Dependency Parsing with Hierarchical Tree LSTMs
- A Latent Variable Model for Generative Dependency Parsing
- Structured language modeling
- [A Fast Unified Model for Parsing and Sentence Understanding](./a-fast-unified-model-for-parsing-and-sentence-understanding.md)
- Relating Probabilistic Grammars and Automata
- [Recurrent Neural Network Regularization](./recurrent-neural-network-regularization.md)
- Inducing History Representations for Broad Coverage Statistical Parsing
- Parsing with Compositional Vector Grammars
- A Neural Syntactic Language Model
- Trainable grammars for speech recognition
- Feature Optimization for Constituent Parsing via Neural Networks
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- Scalable Inference and Training of Context-Rich Syntactic Translation Models
- Probabilistic Top-Down Parsing and Language Modeling
- [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](./improved-semantic-representations-from-tree-structured-long-short-term-memory-networks.md)
- Grammar as a Foreign Language
- A Bayesian Model for Generative Transition-based Dependency Parsing
- Computation of the Probability of Initial Substring Generation by Stochastic Context-Free Grammars
- Robust garden path parsing
- Phrase Dependency Machine Translation with Quasi-Synchronous Tree-to-Tree Features
- Class-Based n-gram Models of Natural Language
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- Discriminative Training of a Neural Network Statistical Parser
- Self-Training PCFG Grammars with Latent Annotations Across Languages
- Joint and Conditional Estimation of Tagging and Parsing Models
- Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing
- [Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network](./feature-rich-part-of-speech-tagging-with-a-cyclic-dependency-network.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- A Maximum-Entropy-Inspired Parser
- Syntactic Processing Using the Generalized Perceptron and Beam Search
- Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems
- Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features
- A Classifier-Based Parser with Linear Run-Time Complexity
- The Neural Network Pushdown Automaton - Architecture, Dynamics and Training
- Effective Self-Training for Parsing
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- Top-Down Nearly-Context-Sensitive Parsing
- Improved Inference for Unlexicalized Parsing
- On the parameter space of generative lexicalized statistical parsing models
- The Penn Chinese TreeBank - Phrase structure annotation of a large corpus
- Efficient Higher-Order CRFs for Morphological Tagging
- LL(*) - the foundation of the ANTLR parser generator
- Transition-Based Dependency Parsing with Stack Long Short-Term Memory
- Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
- Fast and Accurate Shift-Reduce Constituent Parsing
- An efficient context-free parsing algorithm
- An efficient implementation of a new DOP model
- On the Computational Power of Neural Nets
- Finding Structure in Time
- Classes for fast maximum entropy training
- [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](./show-attend-and-tell-neural-image-caption-generation-with-visual-attention.md)
- [Understanding the difficulty of training deep feedforward neural networks](./understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
- A Tutorial on Particle Filtering and Smoothing - Fifteen years later
