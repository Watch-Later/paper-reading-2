---
title: Deep Contextualized Word Representations
authors:
- Matthew E. Peters
- Mark Neumann
- Mohit Iyyer
- Matt Gardner
- Christopher Clark
- Kenton Lee
- Luke Zettlemoyer
fieldsOfStudy:
- Computer Science
meta_key: deep-contextualized-word-representations
numCitedBy: 7987
pdf_relpath: null
ref_count: 65
status: todo
tags:
- gen-from-ref
- paper
venue: NAACL
year: 2018
---

# Deep Contextualized Word Representations

## References

- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- [Semi-supervised sequence tagging with bidirectional language models](./semi-supervised-sequence-tagging-with-bidirectional-language-models.md)
- context2vec - Learning Generic Context Embedding with Bidirectional LSTM
- Word Representations - A Simple and General Method for Semi-Supervised Learning
- Embeddings for Word Sense Disambiguation - An Evaluation Study
- Finding Function in Form - Compositional Character Models for Open Vocabulary Word Representation
- [Character-Aware Neural Language Models](./character-aware-neural-language-models.md)
- Neural Sequence Learning Models for Word Sense Disambiguation
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [Enriching Word Vectors with Subword Information](./enriching-word-vectors-with-subword-information.md)
- Regularizing and Optimizing LSTM Language Models
- Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space
- [Charagram - Embedding Words and Sentences via Character n-grams](./charagram-embedding-words-and-sentences-via-character-n-grams.md)
- [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](./ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- Deep Semantic Role Labeling - What Works and What's Next
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- What do Neural Machine Translation Models Learn about Morphology?
- End-to-end Neural Coreference Resolution
- End-to-end learning of semantic role labeling using recurrent neural networks
- Word Sense Disambiguation - A Unified Evaluation Framework and Empirical Comparison
- Natural Language Inference over Interaction Space
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- Enhanced LSTM for Natural Language Inference
- Towards Robust Linguistic Analysis using OntoNotes
- On the State of the Art of Evaluation in Neural Language Models
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- [A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks](./a-joint-many-task-model-growing-a-neural-network-for-multiple-nlp-tasks.md)
- One billion word benchmark for measuring progress in statistical language modeling
- [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](./end-to-end-sequence-labeling-via-bi-directional-lstm-cnns-crf.md)
- Neural Tree Indexers for Text Understanding
- Learning Global Features for Coreference Resolution
- [Bidirectional Attention Flow for Machine Comprehension](./bidirectional-attention-flow-for-machine-comprehension.md)
- CoNLL-2012 Shared Task - Modeling Multilingual Unrestricted Coreference in OntoNotes
- [Named Entity Recognition with Bidirectional LSTM-CNNs](./named-entity-recognition-with-bidirectional-lstm-cnns.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling
- The Proposition Bank - An Annotated Corpus of Semantic Roles
- [Simple and Effective Multi-Paragraph Reading Comprehension](./simple-and-effective-multi-paragraph-reading-comprehension.md)
- [An Empirical Exploration of Recurrent Network Architectures](./an-empirical-exploration-of-recurrent-network-architectures.md)
- Building a Large Annotated Corpus of English - The Penn Treebank
- [Deep multi-task learning with low level tasks supervised at lower layers](./deep-multi-task-learning-with-low-level-tasks-supervised-at-lower-layers.md)
- Easy Victories and Uphill Battles in Coreference Resolution
- [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](./a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
- [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](./on-the-properties-of-neural-machine-translation-encoder-decoder-approaches.md)
- Deep Reinforcement Learning for Mention-Ranking Coreference Models
- [Gated Self-Matching Networks for Reading Comprehension and Question Answering](./gated-self-matching-networks-for-reading-comprehension-and-question-answering.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Stochastic Answer Networks for Machine Reading Comprehension
- Using a Semantic Concordance for Sense Identification
- [Conditional Random Fields - Probabilistic Models for Segmenting and Labeling Sequence Data](./conditional-random-fields-probabilistic-models-for-segmenting-and-labeling-sequence-data.md)
- [Training Very Deep Networks](./training-very-deep-networks.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Layer Normalization](./layer-normalization.md)
- Introduction to the CoNLL-2003 Shared Task - Language-Independent Named Entity Recognition
- [Neural Architectures for Named Entity Recognition](./neural-architectures-for-named-entity-recognition.md)
- [ADADELTA - An Adaptive Learning Rate Method](./adadelta-an-adaptive-learning-rate-method.md)
