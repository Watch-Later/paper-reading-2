---
title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
authors:
- Y. Gal
- Zoubin Ghahramani
fieldsOfStudy:
- Computer Science
meta_key: a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks
numCitedBy: 1315
pdf_relpath: null
ref_count: 36
status: todo
tags:
- gen-from-ref
- paper
venue: NIPS
year: 2016
---

# A Theoretically Grounded Application of Dropout in Recurrent Neural Networks

## References

- On Fast Dropout and its Applicability to Recurrent Networks
- Dropout as a Bayesian Approximation - Representing Model Uncertainty in Deep Learning
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference
- [LSTM Neural Networks for Language Modeling](./lstm-neural-networks-for-language-modeling.md)
- Bayesian recurrent neural network language model
- Regularization and nonlinearities for neural language models - when are they needed?
- [Speech recognition with deep recurrent neural networks](./speech-recognition-with-deep-recurrent-neural-networks.md)
- RNNDROP - A novel dropout for RNNS in ASR
- [Practical Variational Inference for Neural Networks](./practical-variational-inference-for-neural-networks.md)
- Recognizing recurrent neural networks (rRNN) - Bayesian inference for recurrent neural networks
- Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks
- [Stochastic Backpropagation and Approximate Inference in Deep Generative Models](./stochastic-backpropagation-and-approximate-inference-in-deep-generative-models.md)
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- Bayesian dark knowledge
- Bayesian Dark Knowledge
- [Recurrent Neural Network Regularization](./recurrent-neural-network-regularization.md)
- Where to apply dropout in recurrent neural networks for handwriting recognition?
- Variational Dropout and the Local Reparameterization Trick
- Ensemble learning in Bayesian neural networks
- Dropout Improves Recurrent Neural Networks for Handwriting Recognition
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- Weight Uncertainty in Neural Networks
- Weight Uncertainty in Neural Network
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Bayesian Learning for Neural Networks
- A Practical Bayesian Framework for Backpropagation Networks
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- [Recurrent Continuous Translation Models](./recurrent-continuous-translation-models.md)
- Keeping the neural networks simple by minimizing the description length of the weights
- Seeing Stars - Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales
- Building a Large Annotated Corpus of English - The Penn Treebank
