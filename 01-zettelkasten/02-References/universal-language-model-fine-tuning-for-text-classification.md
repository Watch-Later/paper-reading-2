---
title: Universal Language Model Fine-tuning for Text Classification
authors:
- Jeremy Howard
- Sebastian Ruder
fieldsOfStudy:
- Computer Science
meta_key: universal-language-model-fine-tuning-for-text-classification
numCitedBy: 2251
pdf_relpath: null
ref_count: 57
status: todo
tags:
- gen-from-ref
- paper
venue: ACL
year: 2018
---

# Universal Language Model Fine-tuning for Text Classification

## References

- [Semi-supervised sequence tagging with bidirectional language models](./semi-supervised-sequence-tagging-with-bidirectional-language-models.md)
- Semi-supervised Multitask Learning for Sequence Labeling
- Question Answering through Transfer Learning from Large Fine-grained Supervision Data
- Empower Sequence Labeling with Task-Aware Neural Language Model
- [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](./supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data.md)
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings
- Improving Neural Machine Translation Models with Monolingual Data
- Regularizing and Optimizing LSTM Language Models
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- How Transferable are Neural Networks in NLP Applications?
- [Pointer Sentinel Mixture Models](./pointer-sentinel-mixture-models.md)
- [Adversarial Training Methods for Semi-Supervised Text Classification](./adversarial-training-methods-for-semi-supervised-text-classification.md)
- [Deep Biaffine Attention for Neural Dependency Parsing](./deep-biaffine-attention-for-neural-dependency-parsing.md)
- Exploring the Limits of Weakly Supervised Pretraining
- Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings
- Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling
- Biographies, Bollywood, Boom-boxes and Blenders - Domain Adaptation for Sentiment Classification
- [Learning to Generate Reviews and Discovering Sentiment](./learning-to-generate-reviews-and-discovering-sentiment.md)
- What makes ImageNet good for transfer learning?
- Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies
- UNITN - Training Deep Convolutional Neural Network for Twitter Sentiment Classification
- [Character-level Convolutional Networks for Text Classification](./character-level-convolutional-networks-for-text-classification.md)
- Deep Pyramid Convolutional Neural Networks for Text Categorization
- [Learning Word Vectors for Sentiment Analysis](./learning-word-vectors-for-sentiment-analysis.md)
- Discriminative Neural Sentence Modeling by Tree-Based Convolution
- [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](./decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition.md)
- [A Survey on Transfer Learning](./a-survey-on-transfer-learning.md)
- Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm
- [Fully Convolutional Networks for Semantic Segmentation](./fully-convolutional-networks-for-semantic-segmentation.md)
- Learning Transferable Features with Deep Adaptation Networks
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- Why Does Unsupervised Pre-training Help Deep Learning?
- Multitask Learning - A Knowledge-Based Source of Inductive Bias
- [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](./batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
- GradNorm - Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks
- Colorless Green Recurrent Networks Dream Hierarchically
- [CNN Features Off-the-Shelf - An Astounding Baseline for Recognition](./cnn-features-off-the-shelf-an-astounding-baseline-for-recognition.md)
- [Hypercolumns for object segmentation and fine-grained localization](./hypercolumns-for-object-segmentation-and-fine-grained-localization.md)
- [How transferable are features in deep neural networks?](./how-transferable-are-features-in-deep-neural-networks.md)
- The TREC-8 Question Answering Track Evaluation
- A Model of Inductive Bias Learning
- [Densely Connected Convolutional Networks](./densely-connected-convolutional-networks.md)
- Cyclical Learning Rates for Training Neural Networks
- Deep Boltzmann Machines
- [Greedy Layer-Wise Training of Deep Networks](./greedy-layer-wise-training-of-deep-networks.md)
- Super-convergence - very fast training of neural networks using large learning rates
- [SGDR - Stochastic Gradient Descent with Warm Restarts](./sgdr-stochastic-gradient-descent-with-warm-restarts.md)
- Document categorization in legal electronic discovery - computer classification vs. manual review
- Detecting Automation of Twitter Accounts - Are You a Human, Bot, or Cyborg?
- Review spam detection
- Classifying text messages for the haiti earthquake
- An overview of gradient descent optimization algorithms
- The application of data mining techniques in financial fraud detection - A classification framework and an academic review of literature
- Estimation of Dependences Based on Empirical Data
