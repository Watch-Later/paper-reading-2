---
title: On the difficulty of training recurrent neural networks
authors:
- Razvan Pascanu
- Tomas Mikolov
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: on-the-difficulty-of-training-recurrent-neural-networks
numCitedBy: 3801
pdf_relpath: null
ref_count: 37
status: todo
tags:
- gen-from-ref
- paper
venue: ICML
year: 2013
---

# On the difficulty of training recurrent neural networks

## References

- Learning Recurrent Neural Networks with Hessian-Free Optimization
- The problem of learning long-term dependencies in recurrent networks
- Learning long-term dependencies with gradient descent is difficult
- Advances in optimizing recurrent networks
- New results on recurrent network training - unifying the algorithms and accelerating convergence
- A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
- [Generating Text with Recurrent Neural Networks](./generating-text-with-recurrent-neural-networks.md)
- Learning representations by back-propagating errors
- Training recurrent neural networks
- On the training of recurrent neural networks
- Bifurcations of Recurrent Neural Networks in Gradient Descent Learning
- Reservoir computing approaches to recurrent neural network training
- Optimization and applications of echo state networks with leaky- integrator neurons
- Generalization of backpropagation with application to a recurrent gas market model
- [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](./adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization.md)
- SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
- Long Short-Term Memory in Echo State Networks - Details of a Simulation Study
- [Long Short-Term Memory](./long-short-term-memory.md)
- [Theano - new features and speed improvements](./theano-new-features-and-speed-improvements.md)
- A neurodynamical model for working memory
- On the Computational Power of Neural Nets
- Harnessing Nonlinearity - Predicting Chaotic Systems and Saving Energy in Wireless Communication
- Neural Networks with Adaptive Learning Rate and Momentum Terms
- Finding Structure in Time
- Statistical Language Models Based on Neural Networks
- Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
- Learning representations by backpropagating errors
- Empirical Evaluation and Combination of Advanced Language Modeling Techniques
- Adaptive Synchronization of Neural and Physical Oscillators
- Nonlinear Dynamics and Chaos - With Applications to Physics, Biology, Chemistry and Engineering
- A Novel Connectionist System for Unconstrained Handwriting Recognition
