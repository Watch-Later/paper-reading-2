---
title: Extensions of recurrent neural network language model
authors:
- Tomas Mikolov
- Stefan Kombrink
- L. Burget
- "J. Cernock\xFD"
- S. Khudanpur
fieldsOfStudy:
- Computer Science
meta_key: extensions-of-recurrent-neural-network-language-model
numCitedBy: 1423
pdf_relpath: null
ref_count: 23
status: todo
tags:
- gen-from-ref
- paper
venue: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)
year: 2011
---

# Extensions of recurrent neural network language model

## References

- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- Hierarchical Probabilistic Neural Network Language Model
- Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model
- Training Neural Network Language Models on Very Large Corpora
- Learning representations by back-propagating errors
- Learning long-term dependencies with gradient descent is difficult
- A guide to recurrent neural networks and backpropagation
- Classes for fast maximum entropy training
- Exact training of a neural syntactic language model
- Factored Neural Language Models
- A Neural Syntactic Language Model
- A Neural Probabilistic Language Model
- Finding Structure in Time
- A bit of progress in language modeling
- Neural network based language models for highly inflective languages
- Random forests and the data sparseness problem in language modeling
- Scaling learning algorithms towards AI
- A Joint Language Model With Fine-grain Syntactic Tags
- Learning internal representations by back-propagating errors
