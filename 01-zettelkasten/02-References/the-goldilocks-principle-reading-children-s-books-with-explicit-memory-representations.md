---
title: The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations
authors:
- Felix Hill
- Antoine Bordes
- S. Chopra
- J. Weston
fieldsOfStudy:
- Computer Science
meta_key: the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations
numCitedBy: 531
pdf_relpath: null
ref_count: 40
status: todo
tags:
- gen-from-ref
- paper
venue: ICLR
year: 2016
---

# The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations

## References

- [Memory Networks](./memory-networks.md)
- [Improving Word Representations via Global Context and Multiple Word Prototypes](./improving-word-representations-via-global-context-and-multiple-word-prototypes.md)
- The Microsoft Research Sentence Completion Challenge
- [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](./ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
- A Cache-Based Natural Language Model for Speech Recognition
- Interaction with context during human sentence processing
- MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
- The neurobiology of semantic memory
- [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](./towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks.md)
- Large-scale Simple Question Answering with Memory Networks
- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- Context dependent recurrent neural network language model
- [End-To-End Memory Networks](./end-to-end-memory-networks.md)
- [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](./show-attend-and-tell-neural-image-caption-generation-with-visual-attention.md)
- Large scale image annotation - learning to rank with joint word-image embeddings
- [A Neural Attention Model for Abstractive Sentence Summarization](./a-neural-attention-model-for-abstractive-sentence-summarization.md)
- Learning to Transduce with Unbounded Memory
- Scalable Modified Kneser-Ney Language Model Estimation
- Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
- Word frequency distributions and lexical semantics
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- Learning long-term dependencies with gradient descent is difficult
- Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks
- [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](./published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
- Transition-Based Dependency Parsing with Stack Long Short-Term Memory
- [Simple statistical gradient-following algorithms for connectionist reinforcement learning](./simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning.md)
- [The Stanford CoreNLP Natural Language Processing Toolkit](./the-stanford-corenlp-natural-language-processing-toolkit.md)
- [Regularization of Neural Networks using DropConnect](./regularization-of-neural-networks-using-dropconnect.md)
- Goldilocks and the Three Bears
- [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](./under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
