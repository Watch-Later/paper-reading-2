---
title: Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion

## References

- [Extracting and composing robust features with denoising autoencoders](./extracting-and-composing-robust-features-with-denoising-autoencoders.md)
- Natural Image Denoising with Convolutional Networks
- Why Does Unsupervised Pre-training Help Deep Learning?
- Sparse Feature Learning for Deep Belief Networks
- [A Fast Learning Algorithm for Deep Belief Nets](./a-fast-learning-algorithm-for-deep-belief-nets.md)
- Deep Learning using Robust Interdependent Codes
- Exploring Strategies for Training Deep Neural Networks
- [Greedy Layer-Wise Training of Deep Networks](./greedy-layer-wise-training-of-deep-networks.md)
- Sparse deep belief net model for visual area V2
- Efficient Learning of Sparse Representations with an Energy-Based Model
- Deep learning via semi-supervised embedding
- Training with Noise is Equivalent to Tikhonov Regularization
- An empirical evaluation of deep architectures on problems with many factors of variation
- [Learning Deep Architectures for AI](./learning-deep-architectures-for-ai.md)
- An Information-Maximization Approach to Blind Separation and Blind Deconvolution
- [Reducing the Dimensionality of Data with Neural Networks](./reducing-the-dimensionality-of-data-with-neural-networks.md)
- Neural networks and principal component analysis - Learning from examples without local minima
- Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
- Auto-association by multilayer perceptrons and singular value decomposition
- Emergence of simple-cell receptive field properties by learning a sparse code for natural images
- Scaling learning algorithms towards AI
- Sparse coding with an overcomplete basis set - A strategy employed by V1?
- Creating artificial neural networks that generalize
- Kernel Methods for Deep Learning
- The Effects of Adding Noise During Backpropagation Training on a Generalization Performance
- Using additive noise in back-propagation training
- [Training Products of Experts by Minimizing Contrastive Divergence](./training-products-of-experts-by-minimizing-contrastive-divergence.md)
- The “independent components” of natural scenes are edge filters
- Connectionist Learning Procedures
- Noise Injection - Theoretical Prospects
- Nonlinear Autoassociation Is Not Equivalent to PCA
- Justifying and Generalizing Contrastive Divergence
- Semi-Supervised Learning
- Learning Continuous Attractors in Recurrent Networks
- Backpropagation Applied to Handwritten Zip Code Recognition
- Large-scale kernel machines
- Many-Layered Learning
- Factors influencing learning by backpropagation
- An Application of the Principle of Maximum Information Preservation to Linear Systems
- Incorporating Invariances in Support Vector Learning Machines
- Recognition and Structure from one 2D Model View - Observations on Prototypes, Object Classes and Symmetries
- Receptive fields of single neurones in the cat's striate cortex
- Document image defect models
- Neural networks and physical systems with emergent collective computational abilities.
- Information processing in dynamical systems - foundations of harmony theory
- Parallel distributed processing - explorations in the microstructure of cognition, vol. 1 - foundations
- Statistical Analysis of Non-Lattice Data
- Parallel Distributed Processing - Explorations in the Microstructure of Cognition, vol 1 - Foundations, vol 2 - Psychological and Biological Models
- Dependency Networks for Inference, Collaborative Filtering, and Data Visualization
- Almost optimal lower bounds for small depth circuits
- On the power of small-depth threshold circuits
- PhD thesis - Modeles connexionnistes de l'apprentissage (connectionist learning models)
- Memoires associatives distribuees - Une comparaison (Distributed associative memories - A comparison)
- Emergence of grandmother memory in feed forward networks - learning with noise and forgetfulness
- Deep Learning via Semi-supervised Embedding
- Algorithms for Classifying Recorded Music by Genre
