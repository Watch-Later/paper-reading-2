---
title: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
authors:
- "Djork-Arn\xE9 Clevert"
- Thomas Unterthiner
- S. Hochreiter
fieldsOfStudy:
- Computer Science
meta_key: fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus
numCitedBy: 3669
pdf_relpath: null
ref_count: 74
status: todo
tags:
- gen-from-ref
- paper
venue: ICLR
year: 2016
---

# Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)

## References

- Deep Learning Made Easier by Linear Transformations in Perceptrons
- [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](./batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
- Empirical Evaluation of Rectified Activations in Convolutional Network
- Rectified Factor Networks
- Enhanced Gradient and Adaptive Learning Rate for Training Restricted Boltzmann Machines
- [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](./delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
- [Improving deep neural networks for LVCSR using rectified linear units and dropout](./improving-deep-neural-networks-for-lvcsr-using-rectified-linear-units-and-dropout.md)
- [Network In Network](./network-in-network.md)
- On rectified linear units for speech processing
- [Deeply-Supervised Nets](./deeply-supervised-nets.md)
- Learning Recurrent Neural Networks with Hessian-Free Optimization
- [Long Short-Term Memory](./long-short-term-memory.md)
- Revisiting Natural Gradient for Deep Networks
- Neural Networks - Tricks of the Trade
- [Rectifier Nonlinearities Improve Neural Network Acoustic Models](./rectifier-nonlinearities-improve-neural-network-acoustic-models.md)
- [ImageNet classification with deep convolutional neural networks](./imagenet-classification-with-deep-convolutional-neural-networks.md)
- Natural Neural Networks
- Centering Neural Network Gradient Factors
- Training Neural Networks with Stochastic Hessian-Free Optimization
- [Training Very Deep Networks](./training-very-deep-networks.md)
- [Striving for Simplicity - The All Convolutional Net](./striving-for-simplicity-the-all-convolutional-net.md)
- Learning Semantic Image Representations at a Large Scale
- Krylov Subspace Descent for Deep Learning
- Riemannian metrics for neural networks
- Deep learning via Hessian-free optimization
- The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions
- Fractional Max-Pooling
- Recurrent Neural Net Learning and Vanishing
- Incremental Natural Actor-Critic Algorithms
- Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines
- Natural Gradient Works Efficiently in Learning
- [Maxout Networks](./maxout-networks.md)
- A fast natural Newton method
- Learning to Learn Using Gradient Descent
- Feature Extraction Through LOCOCODE
- [Rectified Linear Units Improve Restricted Boltzmann Machines](./rectified-linear-units-improve-restricted-boltzmann-machines.md)
- DeepTox - Toxicity Prediction using Deep Learning
- Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian-Vector Multiply
- Deep Boltzmann Machines and the Centering Trick
- Adaptive natural gradient learning algorithms for various stochastic models
- Deep Learning as an Opportunity in Virtual Screening
- Statistical Theory of Learning Curves under Entropic Loss Criterion
- Natural Actor-Critic
- Natural Evolution Strategies
- Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix
- [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](./under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
- Topmoumoute Online Natural Gradient Algorithm
- Iterative weighted least squares algorithms for neural networks classifiers
- Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient
- Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
- Toxicity Prediction using Deep Learning
- NATURAL GRADIENT LEARNING WITH A NONHOLONOMIC CONSTRAINT FOR BLIND DECONVOLUTION OF MULTIPLE CHANNELS
- A New Learning Algorithm for Blind Signal Separation
- A Natural Policy Gradient
- Natural Gradient Approach To Blind Separation Of Over- And Under-Complete Mixtures
- [Deep Sparse Rectifier Neural Networks](./deep-sparse-rectifier-neural-networks.md)
- Riemannian metrics for neural networks I - feedforward networks
- Complexity Issues in Natural Gradient Descent Method for Training Multilayer Perceptrons
- A Fast, Compact Approximation of the Exponential Function
- Eigenvalues of covariance matrices - Application to neural-network learning.
- [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](./published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
- Stochastic search using the natural gradient
- Improved Preconditioner for Hessian Free Optimization
- On a Fast, Compact Approximation of the Exponential Function
- Untersuchungen zu dynamischen neuronalen Netzen
- [Efficient BackProp](./efficient-backprop.md)
- Differential-geometrical methods in statistics
