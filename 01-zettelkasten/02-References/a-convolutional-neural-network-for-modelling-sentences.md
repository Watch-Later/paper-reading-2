---
title: A Convolutional Neural Network for Modelling Sentences
authors:
- Nal Kalchbrenner
- Edward Grefenstette
- P. Blunsom
fieldsOfStudy:
- Computer Science
meta_key: a-convolutional-neural-network-for-modelling-sentences
numCitedBy: 2988
pdf_relpath: null
ref_count: 44
status: todo
tags:
- gen-from-ref
- paper
venue: ACL
year: 2014
---

# A Convolutional Neural Network for Modelling Sentences

## References

- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- Recurrent Convolutional Neural Networks for Discourse Compositionality
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- Context dependent recurrent neural network language model
- Grounded Compositional Semantics for Finding and Describing Images with Sentences
- [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](./semi-supervised-recursive-autoencoders-for-predicting-sentiment-distributions.md)
- [Recurrent Continuous Translation Models](./recurrent-continuous-translation-models.md)
- Word Representations - A Simple and General Method for Semi-Supervised Learning
- Inductive Learning in Symbolic Domains Using Structure-Driven Recurrent Neural Networks
- LSTM recurrent networks learn simple context-free and context-sensitive languages
- Continuous Space Translation Models for Phrase-Based Statistical Machine Translation
- Prior Disambiguation of Word Tensors for Constructing Sentence Vectors
- The Role of Syntax in Vector Space Models of Compositional Semantics
- Question Classification using Head Words and their Hypernyms
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- [Extensions of recurrent neural network language model](./extensions-of-recurrent-neural-network-language-model.md)
- From symbolic to sub-symbolic information in question classification
- Experimental Support for a Categorical Compositional Distributional Model of Meaning
- Learning to Map Sentences to Logical Form - Structured Classification with Probabilistic Categorial Grammars
- Learning Question Classifiers
- A Structured Vector Space Model for Word Meaning in Context
- Recursive Distributed Representations
- Phoneme recognition using time-delay neural networks
- Composition in Distributional Models of Semantics
- Connectionist Learning Procedures
- Estimating Linear Models for Compositional Distributional Semantics
- A Context-Theoretic Framework for Compositionality in Distributional Semantics
- [Gradient-based learning applied to document recognition](./gradient-based-learning-applied-to-document-recognition.md)
- Nouns are Vectors, Adjectives are Matrices - Representing Adjective-Noun Constructions in Semantic Space
- Question classification with log-linear models
- Category-theoretic quantitative compositional distributional models of natural language semantics
- Vector-based Models of Semantic Composition
- Vector Space Models of Word Meaning and Phrase Meaning - A Survey
- Mathematical Foundations for a Compositional Distributional Model of Meaning
- Domain and Function - A Dual-Space Model of Semantic Relations and Compositions
- [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](./adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization.md)
- Readings in speech recognition
