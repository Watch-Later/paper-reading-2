---
title: Gaussian Error Linear Units (GELUs)
authors:
- Dan Hendrycks
- Kevin Gimpel
fieldsOfStudy:
- Computer Science
meta_key: gaussian-error-linear-units-gelus
numCitedBy: 971
pdf_relpath: null
ref_count: 26
status: todo
tags:
- gen-from-ref
- paper
venue: ''
year: 2016
---

# Gaussian Error Linear Units (GELUs)

## References

- [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](./fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus.md)
- Adaptive dropout for training deep neural networks
- [Rectified Linear Units Improve Restricted Boltzmann Machines](./rectified-linear-units-improve-restricted-boltzmann-machines.md)
- Natural Neural Networks
- Residual Networks are Exponential Ensembles of Relatively Shallow Networks
- [Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations](./zoneout-regularizing-rnns-by-randomly-preserving-hidden-activations.md)
- A logical calculus of the ideas immanent in nervous activity
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- Neural networks and physical systems with emergent collective computational abilities.
- Deep Residual Networks with Exponential Linear Unit
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](./exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.md)
- [All you need is a good init](./all-you-need-is-a-good-init.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](./weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md)
- Improving Neural Networks with Dropout
- Residual Networks Behave Like Ensembles of Relatively Shallow Networks
- [Acoustic Modeling Using Deep Belief Networks](./acoustic-modeling-using-deep-belief-networks.md)
- [SGDR - Stochastic Gradient Descent with Warm Restarts](./sgdr-stochastic-gradient-descent-with-warm-restarts.md)
- Learning with Pseudo-Ensembles
- Adjusting for Dropout Variance in Batch Normalization and Weight Initialization
- [Wide Residual Networks](./wide-residual-networks.md)
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](./very-deep-convolutional-networks-for-large-scale-image-recognition.md)
- [Deep Networks with Stochastic Depth](./deep-networks-with-stochastic-depth.md)
- A Simple Approximation to the Area Under Standard Normal Curve
- Generalizing and Improving Weight Initialization
- Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters
