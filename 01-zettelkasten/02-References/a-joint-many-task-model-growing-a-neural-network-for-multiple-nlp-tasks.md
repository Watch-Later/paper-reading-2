---
title: A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks

## References

- [Deep multi-task learning with low level tasks supervised at lower layers](./deep-multi-task-learning-with-low-level-tasks-supervised-at-lower-layers.md)
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](./improved-semantic-representations-from-tree-structured-long-short-term-memory-networks.md)
- ABCNN - Attention-Based Convolutional Neural Network for Modeling Sentence Pairs
- [Multi-task Sequence to Sequence Learning](./multi-task-sequence-to-sequence-learning.md)
- Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- Modelling Sentence Pairs with Tree-structured Attentive Encoder
- Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies
- Stack-propagation - Improved Representation Learning for Syntax
- Tree-to-Sequence Attentional Neural Machine Translation
- Improved Transition-Based Parsing and Tagging with Neural Networks
- What Do Recurrent Neural Network Grammars Learn About Syntax?
- [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](./end-to-end-sequence-labeling-via-bi-directional-lstm-cnns-crf.md)
- [Deep Biaffine Attention for Neural Dependency Parsing](./deep-biaffine-attention-for-neural-dependency-parsing.md)
- Neural Machine Translation with Source-Side Latent Graph Parsing
- Dependency Parsing as Head Selection
- [Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network](./feature-rich-part-of-speech-tagging-with-a-cyclic-dependency-network.md)
- SemEval-2014 Task 1 - Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment
- Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data
- [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](./ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
- Structured Training for Neural Network Transition-Based Parsing
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures](./end-to-end-relation-extraction-using-lstms-on-sequences-and-tree-structures.md)
- Globally Normalized Transition-Based Neural Networks
- Cross-Stitch Networks for Multi-task Learning
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- Finding Function in Form - Compositional Character Models for Open Vocabulary Word Representation
- Easy-First Dependency Parsing with Hierarchical Tree LSTMs
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- Learning with Lookahead - Can History-Based Models Rival Globally Optimized Models?
- [Enriching Word Vectors with Subword Information](./enriching-word-vectors-with-subword-information.md)
- Recurrent Convolutional Neural Networks for Text Classification
- [Charagram - Embedding Words and Sentences via Character n-grams](./charagram-embedding-words-and-sentences-via-character-n-grams.md)
- [Semantic Compositionality through Recursive Matrix-Vector Spaces](./semantic-compositionality-through-recursive-matrix-vector-spaces.md)
- Gated Word-Character Recurrent Language Model
- Progressive Neural Networks
- Learning without Forgetting
- [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](./framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures.md)
- Chunking and Dependency Parsing
- Parsing as Language Modeling
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- Top Accuracy and Fast Dependency Parsing is not a Contradiction
- [Maxout Networks](./maxout-networks.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Word Embedding-based Antonym Detection using Thesauri and Distributional Information
- Transition-Based Dependency Parsing with Stack Long Short-Term Memory
- Dropout Improves Recurrent Neural Networks for Handwriting Recognition
- Efficient Normal-Form Parsing for Combinatory Categorial Grammar
- [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](./under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
- Chunking with Support Vector Machines
- Semi-supervised condensed nearest neighbor for part-of-speech tagging
- Illinois-LH - A Denotational and Distributional Approach to Semantics
