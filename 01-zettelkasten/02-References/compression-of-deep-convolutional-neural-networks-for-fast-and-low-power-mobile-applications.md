---
title: Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications

## References

- Compressing Deep Convolutional Networks using Vector Quantization
- A Deep Neural Network Compression Pipeline - Pruning, Quantization, Huffman Encoding
- Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition
- [Speeding up Convolutional Neural Networks with Low Rank Expansions](./speeding-up-convolutional-neural-networks-with-low-rank-expansions.md)
- Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding
- Improving the speed of neural networks on CPUs
- [Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation](./exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.md)
- Fast Training of Convolutional Networks through FFTs
- Accelerating Very Deep Convolutional Networks for Classification and Detection
- Tensorizing Neural Networks
- An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections
- Efficient and accurate approximations of nonlinear convolutional networks
- Compressing Neural Networks with the Hashing Trick
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](./very-deep-convolutional-networks-for-large-scale-image-recognition.md)
- High Performance Convolutional Neural Networks for Document Processing
- Learning both Weights and Connections for Efficient Neural Network
- [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](./delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
- [Going deeper with convolutions](./going-deeper-with-convolutions.md)
- Fast Neural Networks with Circulant Projections
- [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](./batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
- [Network In Network](./network-in-network.md)
- [Understanding the difficulty of training deep feedforward neural networks](./understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
- [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](./under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
- Predicting Parameters in Deep Learning
- Tensor Decompositions and Applications
- Generalized low rank approximations of matrices
- Non-negative tensor factorization with applications to statistics and computer vision
- Perfect Dimensionality Recovery by Variational Bayesian PCA
- Nonnegative Tucker Decomposition
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- Tensor Rank and the Ill-Posedness of the Best Low-Rank Approximation Problem
- Global analytic solution of fully-observed variational Bayesian matrix factorization
- Automatic relevance determination for multi‐way models
- PARAFAC - parallel factor analysis
- [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](./published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
- Bayesian Interpolation
- A Multilinear Singular Value Decomposition
- Analysis of individual differences in multidimensional scaling via an n-way generalization of “Eckart-Young” decomposition
- Some mathematical notes on three-mode factor analysis
- [Sparse Bayesian Learning and the Relevance Vector Machine](./sparse-bayesian-learning-and-the-relevance-vector-machine.md)
