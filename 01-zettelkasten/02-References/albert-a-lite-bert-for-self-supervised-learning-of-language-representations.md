---
title: ALBERT - A Lite BERT for Self-supervised Learning of Language Representations
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# ALBERT - A Lite BERT for Self-supervised Learning of Language Representations

## References

- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- Well-Read Students Learn Better - On the Importance of Pre-training Compact Models
- Efficient Training of BERT by Progressively Stacking
- [StructBERT - Incorporating Language Structures into Pre-training for Deep Language Understanding](./structbert-incorporating-language-structures-into-pre-training-for-deep-language-understanding.md)
- Well-Read Students Learn Better - The Impact of Student Initialization on Knowledge Distillation
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- [BAM! Born-Again Multi-Task Networks for Natural Language Understanding](./bam-born-again-multi-task-networks-for-natural-language-understanding.md)
- Adaptive Input Representations for Neural Language Modeling
- Patient Knowledge Distillation for BERT Model Compression
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](./exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](./transformer-xl-attentive-language-models-beyond-a-fixed-length-context.md)
- Megatron-LM - Training Multi-Billion Parameter Language Models Using Model Parallelism
- [SpanBERT - Improving Pre-training by Representing and Predicting Spans](./spanbert-improving-pre-training-by-representing-and-predicting-spans.md)
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- Modeling Recurrence for Transformer
- Generating Long Sequences with Sparse Transformers
- Reducing BERT Pre-Training Time from 3 Days to 76 Minutes
- [A Simple Method for Commonsense Reasoning](./a-simple-method-for-commonsense-reasoning.md)
- [Skip-Thought Vectors](./skip-thought-vectors.md)
- DisSent - Learning Sentence Representations from Explicit Discourse Relations
- Semi-supervised Sequence Learning
- [Universal Language Model Fine-tuning for Text Classification](./universal-language-model-fine-tuning-for-text-classification.md)
- Deep Equilibrium Models
- Training Deep Nets with Sublinear Memory Cost
- Efficient softmax approximation for GPUs
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- Understanding the Disharmony Between Dropout and Batch Normalization by Variance Shift
- [Learning Distributed Representations of Sentences from Unlabelled Data](./learning-distributed-representations-of-sentences-from-unlabelled-data.md)
- Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](./inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- Learning Generic Sentence Representations Using Convolutional Neural Networks
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](./a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference.md)
- [Distributed Representations of Sentences and Documents](./distributed-representations-of-sentences-and-documents.md)
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- [Mesh-TensorFlow - Deep Learning for Supercomputers](./mesh-tensorflow-deep-learning-for-supercomputers.md)
- [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](./semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation.md)
- The Reversible Residual Network - Backpropagation Without Storing Activations
- [RACE - Large-scale ReAding Comprehension Dataset From Examinations](./race-large-scale-reading-comprehension-dataset-from-examinations.md)
- SentencePiece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing
- [Know What You Don't Know - Unanswerable Questions for SQuAD](./know-what-you-don-t-know-unanswerable-questions-for-squad.md)
- [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](./aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books.md)
- [Neural Network Acceptability Judgments](./neural-network-acceptability-judgments.md)
- The Sixth PASCAL Recognizing Textual Entailment Challenge
- Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning
- Dual Co-Matching Network for Multi-choice Reading Comprehension
- The Seventh PASCAL Recognizing Textual Entailment Challenge
- The Second PASCAL Recognising Textual Entailment Challenge
- Automatically Constructing a Corpus of Sentential Paraphrases
- The PASCAL Recognising Textual Entailment Challenge
- The Winograd Schema Challenge
- [Gaussian Error Linear Units (GELUs)](./gaussian-error-linear-units-gelus.md)
- Centering - A Framework for Modeling the Local Coherence of Discourse
- Coherence and Coreference
- Association for Computational Linguistics
- The Third PASCAL Recognizing Textual Entailment Challenge
