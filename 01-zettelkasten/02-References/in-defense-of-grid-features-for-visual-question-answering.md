---
title: In Defense of Grid Features for Visual Question Answering
authors:
- Huaizu Jiang
- Ishan Misra
- Marcus Rohrbach
- E. Learned-Miller
- Xinlei Chen
fieldsOfStudy:
- Computer Science
meta_key: in-defense-of-grid-features-for-visual-question-answering
numCitedBy: 112
pdf_relpath: null
ref_count: 64
status: todo
tags:
- gen-from-ref
- paper
venue: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2020
---

# In Defense of Grid Features for Visual Question Answering

## References

- [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](./bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.md)
- [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](./making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering.md)
- Answer Them All! Toward Universal Visual Question Answering Models
- [Learning to Count Objects in Natural Images for Visual Question Answering](./learning-to-count-objects-in-natural-images-for-visual-question-answering.md)
- [Deep Modular Co-Attention Networks for Visual Question Answering](./deep-modular-co-attention-networks-for-visual-question-answering.md)
- [Beyond Bilinear - Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering](./beyond-bilinear-generalized-multimodal-factorized-high-order-pooling-for-visual-question-answering.md)
- [Unified Vision-Language Pre-Training for Image Captioning and VQA](./unified-vision-language-pre-training-for-image-captioning-and-vqa.md)
- [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](./multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding.md)
- [Towards VQA Models That Can Read](./towards-vqa-models-that-can-read.md)
- [Tips and Tricks for Visual Question Answering - Learnings from the 2017 Challenge](./tips-and-tricks-for-visual-question-answering-learnings-from-the-2017-challenge.md)
- [VQA - Visual Question Answering](./vqa-visual-question-answering.md)
- [Show and tell - A neural image caption generator](./show-and-tell-a-neural-image-caption-generator.md)
- [Feature Pyramid Networks for Object Detection](./feature-pyramid-networks-for-object-detection.md)
- VizWiz Grand Challenge - Answering Visual Questions from Blind People
- Interpretable Counting for Visual Question Answering
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](./very-deep-convolutional-networks-for-large-scale-image-recognition.md)
- [UNITER - Learning UNiversal Image-TExt Representations](./uniter-learning-universal-image-text-representations.md)
- [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](./visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations.md)
- [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](./vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.md)
- [UNITER - UNiversal Image-TExt Representation Learning](./uniter-universal-image-text-representation-learning.md)
- [VisualBERT - A Simple and Performant Baseline for Vision and Language](./visualbert-a-simple-and-performant-baseline-for-vision-and-language.md)
- [Pythia v0.1 - the Winning Entry to the VQA Challenge 2018](./pythia-v0-1-the-winning-entry-to-the-vqa-challenge-2018.md)
- [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](./vl-bert-pre-training-of-generic-visual-linguistic-representations.md)
- [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](./show-attend-and-tell-neural-image-caption-generation-with-visual-attention.md)
- [Stacked Attention Networks for Image Question Answering](./stacked-attention-networks-for-image-question-answering.md)
- [FiLM - Visual Reasoning with a General Conditioning Layer](./film-visual-reasoning-with-a-general-conditioning-layer.md)
- [Neural-Symbolic VQA - Disentangling Reasoning from Vision and Language Understanding](./neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Aggregated Residual Transformations for Deep Neural Networks](./aggregated-residual-transformations-for-deep-neural-networks.md)
- [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](./faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.md)
- [Bilinear Attention Networks](./bilinear-attention-networks.md)
- [Microsoft COCO - Common Objects in Context](./microsoft-coco-common-objects-in-context.md)
- [Long-term recurrent convolutional networks for visual recognition and description](./long-term-recurrent-convolutional-networks-for-visual-recognition-and-description.md)
- [CIDEr - Consensus-based image description evaluation](./cider-consensus-based-image-description-evaluation.md)
- [SSD - Single Shot MultiBox Detector](./ssd-single-shot-multibox-detector.md)
- [Focal Loss for Dense Object Detection](./focal-loss-for-dense-object-detection.md)
- [Focal Loss for Dense Object Detection](./focal-loss-for-dense-object-detection.md)
- [Deep Visual-Semantic Alignments for Generating Image Descriptions](./deep-visual-semantic-alignments-for-generating-image-descriptions.md)
- [Deformable ConvNets V2 - More Deformable, Better Results](./deformable-convnets-v2-more-deformable-better-results.md)
- [SPICE - Semantic Propositional Image Caption Evaluation](./spice-semantic-propositional-image-caption-evaluation.md)
- [Pyramid Scene Parsing Network](./pyramid-scene-parsing-network.md)
- [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](./lxmert-learning-cross-modality-encoder-representations-from-transformers.md)
- [A simple neural network module for relational reasoning](./a-simple-neural-network-module-for-relational-reasoning.md)
- [CLEVR - A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](./clevr-a-diagnostic-dataset-for-compositional-language-and-elementary-visual-reasoning.md)
- Unified Perceptual Parsing for Scene Understanding
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- [ImageNet - A large-scale hierarchical image database](./imagenet-a-large-scale-hierarchical-image-database.md)
- ReferItGame - Referring to Objects in Photographs of Natural Scenes
- Exploring Nearest Neighbor Approaches for Image Captioning
- [Microsoft COCO Captions - Data Collection and Evaluation Server](./microsoft-coco-captions-data-collection-and-evaluation-server.md)
- METEOR - An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments
- [Bleu - a Method for Automatic Evaluation of Machine Translation](./bleu-a-method-for-automatic-evaluation-of-machine-translation.md)
- [YFCC100M - the new data in multimedia research](./yfcc100m-the-new-data-in-multimedia-research.md)
- Symbol grounding problem
- [Mask R-CNN](./mask-r-cnn.md)
