---
title: Attention is All you Need
authors:
- Ashish Vaswani
- Noam M. Shazeer
- Niki Parmar
- Jakob Uszkoreit
- Llion Jones
- Aidan N. Gomez
- Lukasz Kaiser
- Illia Polosukhin
fieldsOfStudy:
- Computer Science
meta_key: attention-is-all-you-need
numCitedBy: 35157
pdf_relpath: pdfs/Attention%20Is%20All%20You%20Need.pdf
ref_count: 44
status: todo
tags:
- paper
- gen-from-ref
- transformer
- nlp
venue: NIPS
year: 2017
---

[local pdf](../../../pdfs/Attention%20Is%20All%20You%20Need.pdf)

# Attention is All you Need

## 阅读价值

开创了 MLP、CNN、RNN 之后的第 4 大类模型。

## References

- [Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation](./deep-recurrent-models-with-fast-forward-connections-for-neural-machine-translation.md)
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [Neural Machine Translation in Linear Time](./neural-machine-translation-in-linear-time.md)
- [A Deep Reinforced Model for Abstractive Summarization](./a-deep-reinforced-model-for-abstractive-summarization.md)
- Can Active Memory Replace Attention?
- [End-To-End Memory Networks](./end-to-end-memory-networks.md)
- Structured Attention Networks
- [Multi-task Sequence to Sequence Learning](./multi-task-sequence-to-sequence-learning.md)
- [Convolutional Sequence to Sequence Learning](./convolutional-sequence-to-sequence-learning.md)
- [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](./outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer.md)
- Massive Exploration of Neural Machine Translation Architectures
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- [Rethinking the Inception Architecture for Computer Vision](./rethinking-the-inception-architecture-for-computer-vision.md)
- [Neural Machine Translation of Rare Words with Subword Units](./neural-machine-translation-of-rare-words-with-subword-units.md)
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](./empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [Xception - Deep Learning with Depthwise Separable Convolutions](./xception-deep-learning-with-depthwise-separable-convolutions.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- [Long Short-Term Memory-Networks for Machine Reading](./long-short-term-memory-networks-for-machine-reading.md)
- [Neural GPUs Learn Algorithms](./neural-gpus-learn-algorithms.md)
- [Using the Output Embedding to Improve Language Models](./using-the-output-embedding-to-improve-language-models.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- [A Structured Self-attentive Sentence Embedding](./a-structured-self-attentive-sentence-embedding.md)
- Self-Training PCFG Grammars with Latent Annotations Across Languages
- Grammar as a Foreign Language
- Fast and Accurate Shift-Reduce Constituent Parsing
- Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
- Factorization tricks for LSTM networks
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- Learning Accurate, Compact, and Interpretable Tree Annotation
- [Recurrent Neural Network Grammars](./recurrent-neural-network-grammars.md)
- Building a Large Annotated Corpus of English - The Penn Treebank
- Effective Self-Training for Parsing
