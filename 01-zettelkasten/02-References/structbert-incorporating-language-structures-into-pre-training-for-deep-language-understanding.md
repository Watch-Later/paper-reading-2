---
title: StructBERT - Incorporating Language Structures into Pre-training for Deep Language Understanding
authors:
- Wei Wang
- Bin Bi
- Ming Yan
- Chen Wu
- Zuyi Bao
- Liwei Peng
- Luo Si
fieldsOfStudy:
- Computer Science
meta_key: structbert-incorporating-language-structures-into-pre-training-for-deep-language-understanding
numCitedBy: 140
pdf_relpath: null
ref_count: 62
status: todo
tags:
- gen-from-ref
- paper
venue: ICLR
year: 2020
---

# StructBERT - Incorporating Language Structures into Pre-training for Deep Language Understanding

## References

- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- [Multi-Task Deep Neural Networks for Natural Language Understanding](./multi-task-deep-neural-networks-for-natural-language-understanding.md)
- [Sentence Encoders on STILTs - Supplementary Training on Intermediate Labeled-data Tasks](./sentence-encoders-on-stilts-supplementary-training-on-intermediate-labeled-data-tasks.md)
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- [QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension](./qanet-combining-local-convolution-with-global-self-attention-for-reading-comprehension.md)
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](./a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference.md)
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- Discriminative Syntax-Based Word Ordering for Text Generation
- I Know What You Want - Semantic Learning for Text Comprehension
- Word Ordering Without Syntax
- [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](./semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation.md)
- Snorkel - Rapid Training Data Creation with Weak Supervision
- Neural Sentence Ordering
- [SpanBERT - Improving Pre-training by Representing and Predicting Spans](./spanbert-improving-pre-training-by-representing-and-predicting-spans.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- A Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization
- A Comparison of Neural Models for Word Ordering
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- The Sixth PASCAL Recognizing Textual Entailment Challenge
- Building applied natural language generation systems
- [Neural Network Acceptability Judgments](./neural-network-acceptability-judgments.md)
- [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](./aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books.md)
- The Seventh PASCAL Recognizing Textual Entailment Challenge
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Finding Structure in Time
- Automatically Constructing a Corpus of Sentential Paraphrases
- The Winograd Schema Challenge
- [Gaussian Error Linear Units (GELUs)](./gaussian-error-linear-units-gelus.md)
