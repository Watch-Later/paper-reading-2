---
title: Transformers - State-of-the-Art Natural Language Processing
authors:
- Thomas Wolf
- Lysandre Debut
- Victor Sanh
- Julien Chaumond
- Clement Delangue
- Anthony Moi
- Pierric Cistac
- T. Rault
- "R\xE9mi Louf"
- Morgan Funtowicz
- Jamie Brew
fieldsOfStudy:
- Computer Science
meta_key: transformers-state-of-the-art-natural-language-processing
numCitedBy: 2300
pdf_relpath: null
ref_count: 48
status: todo
tags:
- gen-from-ref
- paper
venue: EMNLP
year: 2020
---

# Transformers - State-of-the-Art Natural Language Processing

## References

- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [AllenNLP - A Deep Semantic Natural Language Processing Platform](./allennlp-a-deep-semantic-natural-language-processing-platform.md)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](./exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer.md)
- Reformer - The Efficient Transformer
- [DistilBERT, a distilled version of BERT - smaller, faster, cheaper and lighter](./distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter.md)
- [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](./transformer-xl-attentive-language-models-beyond-a-fixed-length-context.md)
- [SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems](./superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.md)
- [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](./albert-a-lite-bert-for-self-supervised-learning-of-language-representations.md)
- FLAIR - An Easy-to-Use Framework for State-of-the-Art NLP
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- FlauBERT - Unsupervised Language Model Pre-training for French
- [Attention is All you Need](./attention-is-all-you-need.md)
- [Universal Language Model Fine-tuning for Text Classification](./universal-language-model-fine-tuning-for-text-classification.md)
- [SciBERT - A Pretrained Language Model for Scientific Text](./scibert-a-pretrained-language-model-for-scientific-text.md)
- exBERT - A Visual Analysis Tool to Explore Learned Representations in Transformer Models
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- Megatron-LM - Training Multi-Billion Parameter Language Models Using Model Parallelism
- [Cross-lingual Language Model Pretraining](./cross-lingual-language-model-pretraining.md)
- Longformer - The Long-Document Transformer
- [BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](./bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension.md)
- Stanza - A Python Natural Language Processing Toolkit for Many Human Languages
- jiant - A Software Toolkit for Research on General-Purpose Text Understanding Models
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- BERT Rediscovers the Classical NLP Pipeline
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- [ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators](./electra-pre-training-text-encoders-as-discriminators-rather-than-generators.md)
- COMET - Commonsense Transformers for Automatic Knowledge Graph Construction
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- Texar - A Modularized, Versatile, and Extensible Toolkit for Text Generation
- [OpenNMT - Open-Source Toolkit for Neural Machine Translation](./opennmt-open-source-toolkit-for-neural-machine-translation.md)
- [The Stanford CoreNLP Natural Language Processing Toolkit](./the-stanford-corenlp-natural-language-processing-toolkit.md)
- fairseq - A Fast, Extensible Toolkit for Sequence Modeling
- [SpanBERT - Improving Pre-training by Representing and Predicting Spans](./spanbert-improving-pre-training-by-representing-and-predicting-spans.md)
- LSTMVis - A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks
- [NLTK - The Natural Language Toolkit](./nltk-the-natural-language-toolkit.md)
- TVM - An Automated End-to-End Optimizing Compiler for Deep Learning
- Supervised Multimodal Bitransformers for Classifying Images and Text
- Model Cards for Model Reporting
