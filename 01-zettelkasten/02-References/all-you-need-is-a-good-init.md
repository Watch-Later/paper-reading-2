---
title: All you need is a good init
authors:
- Dmytro Mishkin
- Jiri Matas
fieldsOfStudy:
- Computer Science
meta_key: all-you-need-is-a-good-init
numCitedBy: 489
pdf_relpath: null
ref_count: 43
status: todo
tags:
- gen-from-ref
- paper
venue: ICLR
year: 2016
---

# All you need is a good init

## References

- [Deeply-Supervised Nets](./deeply-supervised-nets.md)
- [FitNets - Hints for Thin Deep Nets](./fitnets-hints-for-thin-deep-nets.md)
- [Striving for Simplicity - The All Convolutional Net](./striving-for-simplicity-the-all-convolutional-net.md)
- Batch-normalized Maxout Network in Network
- [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](./delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
- [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](./batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [ImageNet classification with deep convolutional neural networks](./imagenet-classification-with-deep-convolutional-neural-networks.md)
- [Going deeper with convolutions](./going-deeper-with-convolutions.md)
- Very deep multilingual convolutional neural networks for LVCSR
- [Greedy Layer-Wise Training of Deep Networks](./greedy-layer-wise-training-of-deep-networks.md)
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](./very-deep-convolutional-networks-for-large-scale-image-recognition.md)
- Fractional Max-Pooling
- [Learning Multiple Layers of Features from Tiny Images](./learning-multiple-layers-of-features-from-tiny-images.md)
- Random Walk Initialization for Training Very Deep Feedforward Networks
- [Understanding the difficulty of training deep feedforward neural networks](./understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
- [Caffe - Convolutional Architecture for Fast Feature Embedding](./caffe-convolutional-architecture-for-fast-feature-embedding.md)
- [Maxout Networks](./maxout-networks.md)
- [Distilling the Knowledge in a Neural Network](./distilling-the-knowledge-in-a-neural-network.md)
- Spatially-sparse convolutional neural networks
- [Training Very Deep Networks](./training-very-deep-networks.md)
- [Gradient-based learning applied to document recognition](./gradient-based-learning-applied-to-document-recognition.md)
- [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](./under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](./exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.md)
- [Rectifier Nonlinearities Improve Neural Network Acoustic Models](./rectifier-nonlinearities-improve-neural-network-acoustic-models.md)
- [ImageNet Large Scale Visual Recognition Challenge](./imagenet-large-scale-visual-recognition-challenge.md)
- [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](./published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
- [Deep Sparse Rectifier Neural Networks](./deep-sparse-rectifier-neural-networks.md)
- The Technical Details
