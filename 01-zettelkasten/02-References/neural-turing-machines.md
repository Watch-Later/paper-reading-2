---
title: Neural Turing Machines
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Neural Turing Machines

## References

- On the Computational Power of Neural Nets
- Neural networks and physical systems with emergent collective computational abilities.
- Learning Context-free Grammars - Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory
- Recursive Distributed Representations
- Simple Substrates for Complex Cognition
- The Algebraic Mind - Integrating Connectionism and Cognitive Science
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- Continuous attractors and oculomotor control
- BoltzCONS - Dynamic Symbol Structures in a Connectionist Network
- A general framework for adaptive processing of data structures
- Cellular basis of working memory
- Banishing the homunculus - Making working memory work
- [Generating Text with Recurrent Neural Networks](./generating-text-with-recurrent-neural-networks.md)
- Learning to Learn Using Gradient Descent
- Computation Finite And Infinite Machines
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- Synaptic Basis of Cortical Persistent Activity - the Importance of NMDA Receptors to Working Memory
- [Speech recognition with deep recurrent neural networks](./speech-recognition-with-deep-recurrent-neural-networks.md)
- Holographic Reduced Representation - Distributed Representation for Cognitive Structures
- Connectionism and cognitive architecture - A critical analysis
- Memory and the Computational Brain - Why Cognitive Science will Transform Neuroscience
- The magical number seven plus or minus two - some limits on our capacity for processing information.
- [Long Short-Term Memory](./long-short-term-memory.md)
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [Towards End-To-End Speech Recognition with Recurrent Neural Networks](./towards-end-to-end-speech-recognition-with-recurrent-neural-networks.md)
- The importance of mixed selectivity in complex cognitive tasks
- Hyperdimensional Computing - An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors
- The cognitive revolution - a historical perspective
- The Problem of Rapid Variable Creation
- Time constraints and resource sharing in adults' working memory spans.
- Three Models for the Description of Language
- Machine learning - a probabilistic perspective
- [Semantic Compositionality through Recursive Matrix-Vector Spaces](./semantic-compositionality-through-recursive-matrix-vector-spaces.md)
- Memory
- How to Build a Brain - A Neural Architecture for Biological Cognition
- The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky)
- Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems
- First draft of a report on the EDVAC
- Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
- The evolution of the language faculty - Clarifications and implications
- Learning distributed representations of concepts.
- [Parallel & distributed processing](./parallel-distributed-processing.md)
