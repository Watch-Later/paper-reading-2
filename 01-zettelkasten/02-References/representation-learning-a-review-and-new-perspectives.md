---
title: Representation Learning - A Review and New Perspectives
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Representation Learning - A Review and New Perspectives

## References

- Deep Learning of Representations for Unsupervised and Transfer Learning
- Unsupervised and Transfer Learning Challenge - a Deep Learning Approach
- Sparse Feature Learning for Deep Belief Networks
- [Extracting and composing robust features with denoising autoencoders](./extracting-and-composing-robust-features-with-denoising-autoencoders.md)
- The Manifold Tangent Classifier
- Why Does Unsupervised Pre-training Help Deep Learning?
- Understanding Representations Learned in Deep Architectures
- Large-Scale Learning of Embeddings with Reconstruction Sampling
- A Generative Process for sampling Contractive Auto-Encoders
- On deep generative models with applications to recognition
- Neural Networks - Tricks of the Trade
- Non-Local Manifold Tangent Learning
- Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
- Unsupervised feature learning for audio classification using convolutional deep belief networks
- Revisiting Natural Gradient for Deep Networks
- Deep Learning Made Easier by Linear Transformations in Perceptrons
- Selecting Receptive Fields in Deep Networks
- Scaling learning algorithms towards AI
- Measuring Invariances in Deep Networks
- Learning Deep Energy Models
- Algorithms for manifold learning
- Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
- Enhanced Gradient and Adaptive Learning Rate for Training Restricted Boltzmann Machines
- [Learning Deep Architectures for AI](./learning-deep-architectures-for-ai.md)
- [Multimodal learning with deep Boltzmann machines](./multimodal-learning-with-deep-boltzmann-machines.md)
- [Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion](./stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion.md)
- [A Fast Learning Algorithm for Deep Belief Nets](./a-fast-learning-algorithm-for-deep-belief-nets.md)
- [Practical Bayesian Optimization of Machine Learning Algorithms](./practical-bayesian-optimization-of-machine-learning-algorithms.md)
- Practical Recommendations for Gradient-Based Training of Deep Architectures
- [Greedy Layer-Wise Training of Deep Networks](./greedy-layer-wise-training-of-deep-networks.md)
- Classification using discriminative restricted Boltzmann machines
- The Curse of Highly Variable Functions for Local Kernel Machines
- Bayesian and L1 Approaches to Sparse Unsupervised Learning
- Modeling pixel means and covariances using factorized third-order boltzmann machines
- Manifold Parzen Windows
- Self-taught learning - transfer learning from unlabeled data
- A Connection Between Score Matching and Denoising Autoencoders
- DECISION TREES DO NOT GENERALIZE TO NEW VARIATIONS
- [Curriculum learning](./curriculum-learning.md)
- [Learning Multiple Layers of Features from Tiny Images](./learning-multiple-layers-of-features-from-tiny-images.md)
- Higher Order Contractive Auto-Encoder
- Efficient Learning of Deep Boltzmann Machines
- The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization
- ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning
- Boosted Backpropagation Learning for Training Deep Modular Networks
- Exploring Strategies for Training Deep Neural Networks
- Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery
- On Contrastive Divergence Learning
- Deep Boltzmann Machines
- Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines
- Shift-Invariance Sparse Coding for Audio Classification
- Inductive Principles for Learning Restricted Boltzmann Machines
- Transforming Auto-Encoders
- Differentiable Sparse Coding
- What regularized auto-encoders learn from the data-generating distribution
- Structured sparsity through convex optimization
- Domain Adaptation for Statistical Classifiers
- Non-Local Manifold Parzen Windows
- Efficient Learning of Sparse Representations with an Energy-Based Model
- Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders
- Natural Image Denoising with Convolutional Networks
- Connectionist Learning of Belief Networks
- Unsupervised Learning of Sparse Features for Scalable Audio Classification
- Algorithms for Hyper-Parameter Optimization
- Learning a Parametric Embedding by Preserving Local Structure
- [Understanding the difficulty of training deep feedforward neural networks](./understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
- [Contractive Auto-Encoders - Explicit Invariance During Feature Extraction](./contractive-auto-encoders-explicit-invariance-during-feature-extraction.md)
- Learning the 2-D Topology of Images
- Learning Sparse Topographic Representations with Products of Student-t Distributions
- Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images
- Learning Convolutional Feature Hierarchies for Visual Recognition
- Learning Transformational Invariants from Natural Movies
- The Recurrent Temporal Restricted Boltzmann Machine
- Sparse deep belief net model for visual area V2
- Convolutional Learning of Spatio-temporal Features
- [Training Products of Experts by Minimizing Contrastive Divergence](./training-products-of-experts-by-minimizing-contrastive-divergence.md)
- [Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis](./learning-hierarchical-invariant-spatio-temporal-features-for-action-recognition-with-independent-subspace-analysis.md)
- On optimization methods for deep learning
- Learning Recurrent Neural Networks with Hessian-Free Optimization
- [Domain Adaptation for Large-Scale Sentiment Classification - A Deep Learning Approach](./domain-adaptation-for-large-scale-sentiment-classification-a-deep-learning-approach.md)
- Better Mixing via Deep Representations
- [Reducing the Dimensionality of Data with Neural Networks](./reducing-the-dimensionality-of-data-with-neural-networks.md)
- Convolutional Deep Belief Networks on CIFAR-10
- Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning
- Deep learning via semi-supervised embedding
- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- Closed-Form EM for Sparse Coding and Its Application to Source Separation
- Deep learning via Hessian-free optimization
- Deep learning from temporal coherence in video
- Large scale image annotation - learning to rank with joint word-image embeddings
- Nonlinear Learning using Local Coordinate Coding
- Natural Gradient Works Efficiently in Learning
- Improved Local Coordinate Coding using Local Tangents
- [Acoustic Modeling Using Deep Belief Networks](./acoustic-modeling-using-deep-belief-networks.md)
- Deep Learning using Robust Interdependent Codes
- Learning Mixture Models of Spatial Coherence
- Regularized estimation of image statistics by Score Matching
- [Laplacian Eigenmaps for Dimensionality Reduction and Data Representation](./laplacian-eigenmaps-for-dimensionality-reduction-and-data-representation.md)
- Learning long-term dependencies with gradient descent is difficult
- Learning Fast Approximations of Sparse Coding
- A general framework for adaptive processing of data structures
- Learning Deep Boltzmann Machines using Adaptive MCMC
- Training recurrent neural networks
- A Closed-Form EM Algorithm for Sparse Coding
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- Empirical Risk Minimization of Graphical Model Parameters Given Approximate Inference, Decoding, and Model Structure
- Sample Complexity of Testing the Manifold Hypothesis
- Ask the locals - Multi-way local pooling for image recognition
- Learning Horizontal Connections in a Sparse Coding Model of Natural Images
- Learning image representations from the pixel level via hierarchical sparse coding
- Restricted Boltzmann machines for collaborative filtering
- On Autoencoders and Score Matching for Energy Based Models
- Marginalized Denoising Autoencoders for Domain Adaptation
- Transforming Autoencoders
- Learning Continuous Attractors in Recurrent Networks
- When Does a Mixture of Products Contain a Product of Mixtures?
- Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields
- Learning Many Related Tasks at the Same Time with Backpropagation
- Multiple Texture Boltzmann Machines
- Generating more realistic images using gated MRF's
- A Theoretical Analysis of Feature Pooling in Visual Recognition
- Inductive Principles for Restricted Boltzmann Machine Learning
- Generative versus discriminative training of RBMs for classification of fMRI images
- A Neural Probabilistic Language Model
- Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine
- Unsupervised Learning of Image Manifolds by Semidefinite Programming
- Factored conditional restricted Boltzmann Machines for modeling motion style
- Tiled convolutional neural networks
- Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
- Noise-contrastive estimation - A new estimation principle for unnormalized statistical models
- Structured Variable Selection with Sparsity-Inducing Norms
- Using fast weights to improve persistent contrastive divergence
- [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](./semi-supervised-recursive-autoencoders-for-predicting-sentiment-distributions.md)
- Learning in Markov Random Fields using Tempered Transitions
- On Training Deep Boltzmann Machines
- Sparse coding with an overcomplete basis set - A strategy employed by V1?
- Justifying and Generalizing Contrastive Divergence
- [A Practical Guide to Training Restricted Boltzmann Machines](./a-practical-guide-to-training-restricted-boltzmann-machines.md)
- Slow Feature Analysis - Unsupervised Learning of Invariances
- Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition
- [ImageNet classification with deep convolutional neural networks](./imagenet-classification-with-deep-convolutional-neural-networks.md)
- Auto-association by multilayer perceptrons and singular value decomposition
- Evaluating probabilities under high-dimensional latent variable models
- Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machines
- From machine learning to machine reasoning
- Neural Networks - Tricks of the Trade
- Slow, Decorrelated Features for Pretraining Complex Cell-like Networks
- [Visualizing Data using t-SNE](./visualizing-data-using-t-sne.md)
- [Nonlinear dimensionality reduction by locally linear embedding.](./nonlinear-dimensionality-reduction-by-locally-linear-embedding.md)
- Best practices for convolutional neural networks applied to visual document analysis
- Emergence of simple-cell receptive field properties by learning a sparse code for natural images
- Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio
- Estimation of Non-Normalized Statistical Models by Score Matching
- Unsupervised learning of visual invariance with temporal coherence
- Robust Object Recognition with Cortex-Like Mechanisms
- On the Expressive Power of Deep Architectures
- Recursive Distributed Representations
- On Tracking The Partition Function
- Quickly Generating Representative Samples from an RBM-Derived Process
- Autoencoders, Minimum Description Length and Helmholtz Free Energy
- Some extensions of score matching
- Unsupervised Models of Images by Spikeand-Slab RBMs
- [Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition](./context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition.md)
- Stochastic Neighbor Embedding
- Structured sparse coding via lateral inhibition
- Deconvolutional networks
- Learning invariant features through topographic filter maps
- The “independent components” of natural scenes are edge filters
- [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](./deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups.md)
- [A global geometric framework for nonlinear dimensionality reduction.](./a-global-geometric-framework-for-nonlinear-dimensionality-reduction.md)
- Self-organizing neural network that discovers surfaces in random-dot stereograms
- Products of Experts
- [Multi-column deep neural networks for image classification](./multi-column-deep-neural-networks-for-image-classification.md)
- What is the best multi-stage architecture for object recognition?
- On the Convergence Properties of Contrastive Divergence
- [Deep Neural Networks for Acoustic Modeling in Speech Recognition](./deep-neural-networks-for-acoustic-modeling-in-speech-recognition.md)
- Semantic hashing
- Sequential Labeling Using Deep-Structured Conditional Random Fields
- Parallel tempering is efficient for learning restricted Boltzmann machines
- Learning Process in an Asymmetric Threshold Network
- Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing
- Optimal Approximation of Signal Priors
- A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning
- Natural Image Statistics - A Probabilistic Approach to Early Computational Vision
- [Rectified Linear Units Improve Restricted Boltzmann Machines](./rectified-linear-units-improve-restricted-boltzmann-machines.md)
- Topmoumoute Online Natural Gradient Algorithm
- Unsupervised Discovery of Nonlinear Structure Using Contrastive Backpropagation
- Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering
- Training restricted Boltzmann machines using approximations to the likelihood gradient
- [Gradient-based learning applied to document recognition](./gradient-based-learning-applied-to-document-recognition.md)
- Slow feature analysis yields a rich repertoire of complex cell properties.
- Should Penalized Least Squares Regression be Interpreted as Maximum A Posteriori Estimation?
- Temporal Coherence, Natural Image Sequences, and the Visual Cortex
- Charting a Manifold
- Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces
- Hessian Eigenmaps - new locally linear embedding techniques for high-dimensional data
- Structured Output Layer Neural Network Language Models for Speech Recognition
- Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation
- Recognition and Structure from one 2D Model View - Observations on Prototypes, Object Classes and Symmetries
- Binary coding of speech spectrograms using a deep auto-encoder
- Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
- Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation
- A Spike and Slab Restricted Boltzmann Machine
- Probabilistic Inference Using Markov Chain Monte Carlo Methods
- A Connectionist Approach to Speech Recognition
- Independent component analysis, A new concept?
- On the training of recurrent neural networks
- Backpropagation Applied to Handwritten Zip Code Recognition
- Probabilistic Principal Component Analysis
- The Convergence of Contrastive Divergences
- Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
- [Object recognition from local scale-invariant features](./object-recognition-from-local-scale-invariant-features.md)
- Large-Scale FPGA-based Convolutional Networks
- Statistical Analysis of Non-Lattice Data
- Herding Dynamic Weights for Partially Observed Random Field Models
- Deep Coding Network
- Hessian eigenmaps - Locally linear embedding techniques for high-dimensional data
- How are complex cell properties adapted to the statistics of natural stimuli?
- How Does the Brain Solve Visual Object Recognition?
- [Beyond Bags of Features - Spatial Pyramid Matching for Recognizing Natural Scene Categories](./beyond-bags-of-features-spatial-pyramid-matching-for-recognizing-natural-scene-categories.md)
- Hierarchical models of object recognition in cortex
- Blind separation of sources, part I - An adaptive algorithm based on neuromimetic architecture
- Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription
- Efficient Pattern Recognition Using a New Transformation Distance
- Information processing in dynamical systems - foundations of harmony theory
- [Deep Sparse Rectifier Neural Networks](./deep-sparse-rectifier-neural-networks.md)
- Empirical Evaluation and Combination of Advanced Language Modeling Techniques
- Projection Pursuit Regression
- [Random Search for Hyper-Parameter Optimization](./random-search-for-hyper-parameter-optimization.md)
- Independent Component Analysis
- [Neocognitron - A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position](./neocognitron-a-self-organizing-neural-network-model-for-a-mechanism-of-pattern-recognition-unaffected-by-shift-in-position.md)
- Asymptotic Efficiency of Deterministic Estimators for Discrete Energy-Based Models - Ratio Matching and Pseudolikelihood
- Conversational Speech Transcription Using Context-Dependent Deep Neural Networks
- Deep, Big, Simple Neural Nets for Handwritten Digit Recognition
- Neocognitron - A new algorithm for pattern recognition tolerant of deformations and shifts in position
- Nonlinear Component Analysis as a Kernel Eigenvalue Problem
- Document image defect models
- Group Invariant Scattering
- Receptive fields of single neurones in the cat's striate cortex
- Classification with scattering operators
- Topographic Independent Component Analysis
- On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates
- Neural net language models
- Almost optimal lower bounds for small depth circuits
- [Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper](./maximum-likelihood-from-incomplete-data-via-the-em-algorithm-plus-discussions-on-the-paper.md)
- On the power of small-depth threshold circuits
- LIII. On lines and planes of closest fit to systems of points in space
- A Ph.D. Thesis
- Independent Component Analysis
- EM Algorithms for PCA and Sensible PCA
- Réseaux de neurones à relaxation entraînés par critère d'autoencodeur débruitant
- [Efficient BackProp](./efficient-backprop.md)
- Analysis of a complex of statistical variables into principal components.
- Natural Gradient Revisited
- Learning processes in an asymmetric threshold network
- Generalization and network design strategies
- Learning distributed representations of concepts.
