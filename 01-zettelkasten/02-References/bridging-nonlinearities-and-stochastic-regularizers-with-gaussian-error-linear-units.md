---
title: Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units
authors:
- Dan Hendrycks
- Kevin Gimpel
fieldsOfStudy:
- Computer Science
meta_key: bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units
numCitedBy: 289
pdf_relpath: null
ref_count: 24
status: todo
tags:
- gen-from-ref
- paper
venue: ArXiv
year: 2016
---

# Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units

## References

- [Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations](./zoneout-regularizing-rnns-by-randomly-preserving-hidden-activations.md)
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](./exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.md)
- [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](./fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus.md)
- [Rectifier Nonlinearities Improve Neural Network Acoustic Models](./rectifier-nonlinearities-improve-neural-network-acoustic-models.md)
- Adaptive dropout for training deep neural networks
- Residual Networks are Exponential Ensembles of Relatively Shallow Networks
- Learning with Pseudo-Ensembles
- Generalizing and Improving Weight Initialization
- Fast dropout training
- [Rectified Linear Units Improve Restricted Boltzmann Machines](./rectified-linear-units-improve-restricted-boltzmann-machines.md)
- [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](./weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- [SGDR - Stochastic Gradient Descent with Warm Restarts](./sgdr-stochastic-gradient-descent-with-warm-restarts.md)
- Improving Neural Networks with Dropout
- Neural networks and physical systems with emergent collective computational abilities.
- Adjusting for Dropout Variance in Batch Normalization and Weight Initialization
- [Deep Networks with Stochastic Depth](./deep-networks-with-stochastic-depth.md)
- [Acoustic Modeling Using Deep Belief Networks](./acoustic-modeling-using-deep-belief-networks.md)
- [All you need is a good init](./all-you-need-is-a-good-init.md)
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](./very-deep-convolutional-networks-for-large-scale-image-recognition.md)
- A Simple Approximation to the Area Under Standard Normal Curve
- Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters
