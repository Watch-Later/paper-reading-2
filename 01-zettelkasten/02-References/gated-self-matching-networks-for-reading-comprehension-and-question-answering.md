---
title: Gated Self-Matching Networks for Reading Comprehension and Question Answering
authors:
- Wenhui Wang
- Nan Yang
- Furu Wei
- Baobao Chang
- M. Zhou
fieldsOfStudy:
- Computer Science
meta_key: gated-self-matching-networks-for-reading-comprehension-and-question-answering
numCitedBy: 573
pdf_relpath: null
ref_count: 38
status: todo
tags:
- gen-from-ref
- paper
venue: ACL
year: 2017
---

# Gated Self-Matching Networks for Reading Comprehension and Question Answering

## References

- Gated-Attention Readers for Text Comprehension
- Learning Recurrent Span Representations for Extractive Question Answering
- Multi-Perspective Context Matching for Machine Comprehension
- [Text Understanding with the Attention Sum Reader Network](./text-understanding-with-the-attention-sum-reader-network.md)
- Attention-over-Attention Neural Networks for Reading Comprehension
- [Bidirectional Attention Flow for Machine Comprehension](./bidirectional-attention-flow-for-machine-comprehension.md)
- [Machine Comprehension Using Match-LSTM and Answer Pointer](./machine-comprehension-using-match-lstm-and-answer-pointer.md)
- Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering
- [Dynamic Coattention Networks For Question Answering](./dynamic-coattention-networks-for-question-answering.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- [Long Short-Term Memory-Networks for Machine Reading](./long-short-term-memory-networks-for-machine-reading.md)
- Natural Language Comprehension with the EpiReader
- [A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task](./a-thorough-examination-of-the-cnn-daily-mail-reading-comprehension-task.md)
- ReasoNet - Learning to Stop Reading in Machine Comprehension
- Words or Characters? Fine-grained Gating for Reading Comprehension
- [MS MARCO - A Human Generated MAchine Reading COmprehension Dataset](./ms-marco-a-human-generated-machine-reading-comprehension-dataset.md)
- MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
- Iterative Alternating Neural Attention for Machine Reading
- Modeling Biological Processes for Reading Comprehension
- [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](./the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations.md)
- WikiQA - A Challenge Dataset for Open-Domain Question Answering
- FastQA - A Simple and Efficient Neural Architecture for Question Answering
- [Learning Natural Language Inference with LSTM](./learning-natural-language-inference-with-lstm.md)
- [Reasoning about Entailment with Neural Attention](./reasoning-about-entailment-with-neural-attention.md)
- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- Not All Contexts Are Created Equal - Better Word Representations with Variable Attention
- [A Decomposable Attention Model for Natural Language Inference](./a-decomposable-attention-model-for-natural-language-inference.md)
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Pointer Networks](./pointer-networks.md)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- [The Stanford CoreNLP Natural Language Processing Toolkit](./the-stanford-corenlp-natural-language-processing-toolkit.md)
- [ADADELTA - An Adaptive Learning Rate Method](./adadelta-an-adaptive-learning-rate-method.md)
- End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking
- Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL
