---
title: Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
authors:
- Yann Dauphin
- Razvan Pascanu
- "\xC7aglar G\xFCl\xE7ehre"
- Kyunghyun Cho
- S. Ganguli
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization
numCitedBy: 1075
pdf_relpath: null
ref_count: 34
status: todo
tags:
- gen-from-ref
- paper
venue: NIPS
year: 2014
---

# Identifying and attacking the saddle point problem in high-dimensional non-convex optimization

## References

- On the saddle point problem for non-convex optimization
- An analysis on negative curvature induced by singularity in multi-layer neural-network learning
- Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](./exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.md)
- Krylov Subspace Descent for Deep Learning
- Newton‚ÄêType Methods
- Replica Symmetry Breaking Condition Exposed by Random Matrix Calculation of Landscape Complexity
- Neural networks and principal component analysis - Learning from examples without local minima
- Fast Exact Multiplication by the Hessian
- [Random Search for Hyper-Parameter Optimization](./random-search-for-hyper-parameter-optimization.md)
- On-Line Learning Theory of Soft Committee Machines with Correlated Hidden Units - Steepest Gradient Descent and Natural Gradient Descent
- Topmoumoute Online Natural Gradient Algorithm
- [On the importance of initialization and momentum in deep learning](./on-the-importance-of-initialization-and-momentum-in-deep-learning.md)
- Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)
- Natural gradient descent for on-line learning
- Deep learning via Hessian-free optimization
- Revisiting Natural Gradient for Deep Networks
- [On the difficulty of training recurrent neural networks](./on-the-difficulty-of-training-recurrent-neural-networks.md)
- On the Distribution of the Roots of Certain Symmetric Matrices
- Statistics of critical points of Gaussian fields on large-dimensional spaces.
- Advanced Calculus - A Geometric View
- Learning hierarchical category structure in deep neural networks
- Learning long-term dependencies with gradient descent is difficult
- [Theano - new features and speed improvements](./theano-new-features-and-speed-improvements.md)
- [Numerical Optimization](./numerical-optimization.md)
- On-line learning in soft committee machines.
