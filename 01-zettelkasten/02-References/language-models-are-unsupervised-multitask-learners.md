---
title: Language Models are Unsupervised Multitask Learners
authors:
- Alec Radford
- Jeff Wu
- Rewon Child
- D. Luan
- Dario Amodei
- Ilya Sutskever
fieldsOfStudy:
- Computer Science
meta_key: language-models-are-unsupervised-multitask-learners
numCitedBy: 6284
pdf_relpath: null
ref_count: 75
status: todo
tags:
- gen-from-ref
- paper
venue: ''
year: 2019
---

# Language Models are Unsupervised Multitask Learners

## References

- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- Dialog-based Language Learning
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning
- [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](./supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data.md)
- Learning and Evaluating General Linguistic Intelligence
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [A Neural Conversational Model](./a-neural-conversational-model.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Unsupervised Pretraining for Sequence to Sequence Learning](./unsupervised-pretraining-for-sequence-to-sequence-learning.md)
- Unsupervised Machine Translation Using Monolingual Corpora Only
- One billion word benchmark for measuring progress in statistical language modeling
- [Universal Language Model Fine-tuning for Text Classification](./universal-language-model-fine-tuning-for-text-classification.md)
- The LAMBADA dataset - Word prediction requiring a broad discourse context
- Unsupervised Neural Machine Translation
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- Word Translation Without Parallel Data
- [A Simple Method for Commonsense Reasoning](./a-simple-method-for-commonsense-reasoning.md)
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- Looking for ELMo's friends - Sentence-Level Pretraining Beyond Language Modeling
- [Multilingual Language Processing From Bytes](./multilingual-language-processing-from-bytes.md)
- [Neural Machine Translation of Rare Words with Subword Units](./neural-machine-translation-of-rare-words-with-subword-units.md)
- An Effective Approach to Unsupervised Machine Translation
- Bottom-Up Abstractive Summarization
- Wizard of Wikipedia - Knowledge-Powered Conversational agents
- [Natural Questions - A Benchmark for Question Answering Research](./natural-questions-a-benchmark-for-question-answering-research.md)
- [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](./transformer-xl-attentive-language-models-beyond-a-fixed-length-context.md)
- CoQA - A Conversational Question Answering Challenge
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- Semi-supervised Sequence Learning
- A Neural Probabilistic Language Model
- Visualizing and Understanding Recurrent Networks
- [Pointer Sentinel Mixture Models](./pointer-sentinel-mixture-models.md)
- [Learning Distributed Representations of Sentences from Unlabelled Data](./learning-distributed-representations-of-sentences-from-unlabelled-data.md)
- [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](./abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond.md)
- [Learning to Generate Reviews and Discovering Sentiment](./learning-to-generate-reviews-and-discovering-sentiment.md)
- TransferTransfo - A Transfer Learning Approach for Neural Network Based Conversational Agents
- [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](./the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations.md)
- [Adversarial Examples for Evaluating Reading Comprehension Systems](./adversarial-examples-for-evaluating-reading-comprehension-systems.md)
- No Training Required - Exploring Random Encoders for Sentence Classification
- Generating Wikipedia by Summarizing Long Sequences
- [Get To The Point - Summarization with Pointer-Generator Networks](./get-to-the-point-summarization-with-pointer-generator-networks.md)
- [Character-Level Language Modeling with Deeper Self-Attention](./character-level-language-modeling-with-deeper-self-attention.md)
- Entity Tracking Improves Cloze-style Reading Comprehension
- FRAGE - Frequency-Agnostic Word Representation
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- Overcoming catastrophic forgetting in neural networks
- Story Cloze Task - UW NLP System
- [Deep Speech 2 - End-to-End Speech Recognition in English and Mandarin](./deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin.md)
- Deep Learning Scaling is Predictable, Empirically
- Do We Train on Test Data? Purging CIFAR of Near-Duplicates
- [Building machines that learn and think like people](./building-machines-that-learn-and-think-like-people.md)
- Hierarchical Neural Story Generation
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](./model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks.md)
- Multitask Learning
- Do CIFAR-10 Classifiers Generalize to CIFAR-10?
- A BERT Baseline for the Natural Questions
- On the Evaluation of Common-Sense Reasoning in Natural Language Understanding
- Embracing data abundance - BookTest Dataset for Reading Comprehension
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- Neural Word Embedding as Implicit Matrix Factorization
- Towards Principled Unsupervised Learning
- Improving Neural Language Models with a Continuous Cache
- [ImageNet classification with deep convolutional neural networks](./imagenet-classification-with-deep-convolutional-neural-networks.md)
- [Identity Mappings in Deep Residual Networks](./identity-mappings-in-deep-residual-networks.md)
- The Winograd Schema Challenge
- [The Natural Language Decathlon - Multitask Learning as Question Answering](./the-natural-language-decathlon-multitask-learning-as-question-answering.md)
- Strike (With) a Pose - Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects
- Content extraction using diverse feature sets
- The advantages and challenges of “big data” - Insights from the 14 billion word iWeb corpus
- Interpolated estimation of Markov source parameters from sparse data
