---
title: Efficient BackProp
authors:
- Yann LeCun
- L. Bottou
- G. Orr
- "K. M\xFCller"
fieldsOfStudy:
- Computer Science
meta_key: efficient-backprop
numCitedBy: 2630
pdf_relpath: null
ref_count: 36
status: todo
tags:
- gen-from-ref
- paper
venue: 'Neural Networks: Tricks of the Trade'
year: 2012
---

# Efficient BackProp

## References

- PhD thesis - Modeles connexionnistes de l'apprentissage (connectionist learning models)
- Matrix computations
- Practical Methods of Optimization
- Mathematical Approaches to Neural Networks
- Generalization and network design strategies
- On-line Learning of Dichotomies - Algorithms and Learning Curves.
- Regularization in the Selection of Radial Basis Function Centers
- Exact solution for on-line learning in multilayer neural networks.
- Adaptive Algorithms and Stochastic Approximations
- Second Order Properties of Error Surfaces
- [Statistical learning theory](./statistical-learning-theory.md)
- Fast Learning in Networks of Locally-Tuned Processing Units
- Supervised Learning On Large Redundant Training Sets
- Increased rates of convergence through learning rate adaptation
- Neural networks for pattern recognition
- Stochastic dynamics of learning with momentum in neural networks
- [The Nature of Statistical Learning Theory](./the-nature-of-statistical-learning-theory.md)
- Phoneme recognition using time-delay neural networks
- Note on Learning Rate Schedules for Stochastic Optimization
- Removing Noise in On-Line Search using Adaptive Batch Sizes
- Efficient Parallel Learning Algorithms for Neural Networks
- Minimisation methods for training feedforward neural networks
- Neural Networks and the Bias/Variance Dilemma
- On-line learning processes in artificial neural networks
- Computing second derivatives in feed-forward networks - a review
- A scaled conjugate gradient algorithm for fast supervised learning
- Complexity Issues in Natural Gradient Descent Method for Training Multilayer Perceptrons
- First- and Second-Order Methods for Learning - Between Steepest Descent and Newton's Method
- Fast Exact Multiplication by the Hessian
- Optimal Brain Damage
- Adapting Bias by Gradient Descent - An Incremental Version of Delta-Bar-Delta
- The Efficiency and the Robustness of Natural Gradient Descent Learning Rule
- Natural Gradient Works Efficiently in Learning
- Adaptive On-line Learning in Changing Environments
- Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors
- Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient
