---
title: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling
authors:
- Shaojie Bai
- J. Z. Kolter
- V. Koltun
fieldsOfStudy:
- Computer Science
meta_key: an-empirical-evaluation-of-generic-convolutional-and-recurrent-networks-for-sequence-modeling
numCitedBy: 1756
pdf_relpath: null
ref_count: 86
status: todo
tags:
- gen-from-ref
- paper
venue: ArXiv
year: 2018
---

# An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling

## References

- [Convolutional Sequence to Sequence Learning](./convolutional-sequence-to-sequence-learning.md)
- [Language Modeling with Gated Convolutional Networks](./language-modeling-with-gated-convolutional-networks.md)
- [A Convolutional Neural Network for Modelling Sentences](./a-convolutional-neural-network-for-modelling-sentences.md)
- Dilated Recurrent Neural Networks
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](./empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [An Empirical Exploration of Recurrent Network Architectures](./an-empirical-exploration-of-recurrent-network-architectures.md)
- Skip RNN - Learning to Skip State Updates in Recurrent Neural Networks
- Quasi-Recurrent Neural Networks
- [Generating Text with Recurrent Neural Networks](./generating-text-with-recurrent-neural-networks.md)
- A Convolutional Encoder Model for Neural Machine Translation
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
- Regularizing and Optimizing LSTM Language Models
- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- Training and Analysing Deep Recurrent Neural Networks
- [Very Deep Convolutional Networks for Text Classification](./very-deep-convolutional-networks-for-text-classification.md)
- Deep Pyramid Convolutional Neural Networks for Text Categorization
- Comparative Study of CNN and RNN for Natural Language Processing
- [Hierarchical Multiscale Recurrent Neural Networks](./hierarchical-multiscale-recurrent-neural-networks.md)
- [Neural Machine Translation in Linear Time](./neural-machine-translation-in-linear-time.md)
- How to Construct Deep Recurrent Neural Networks
- Modeling temporal dependencies in data using a DBN-LSTM
- On the State of the Art of Evaluation in Neural Language Models
- A Clockwork RNN
- Supervised Sequence Labelling with Recurrent Neural Networks
- [Fully Convolutional Networks for Semantic Segmentation](./fully-convolutional-networks-for-semantic-segmentation.md)
- Temporal Convolutional Networks for Action Segmentation and Detection
- Learning Recurrent Neural Networks with Hessian-Free Optimization
- Architectural Complexity Measures of Recurrent Neural Networks
- [Recurrent Batch Normalization](./recurrent-batch-normalization.md)
- [Convolutional Neural Networks for Sentence Classification](./convolutional-neural-networks-for-sentence-classification.md)
- [Pointer Sentinel Mixture Models](./pointer-sentinel-mixture-models.md)
- [On Multiplicative Integration with Recurrent Neural Networks](./on-multiplicative-integration-with-recurrent-neural-networks.md)
- [Multi-Scale Context Aggregation by Dilated Convolutions](./multi-scale-context-aggregation-by-dilated-convolutions.md)
- [LSTM - A Search Space Odyssey](./lstm-a-search-space-odyssey.md)
- [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](./weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md)
- Effective Use of Word Order for Text Categorization with Convolutional Neural Networks
- Regularizing RNNs by Stabilizing Activations
- Learning long-term dependencies with gradient descent is difficult
- Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs
- SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
- Bidirectional recurrent neural networks
- Diagonal rnns in symbolic music modeling
- Convolutional LSTM Network - A Machine Learning Approach for Precipitation Nowcasting
- Unitary Evolution Recurrent Neural Networks
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [Character-level Convolutional Networks for Text Classification](./character-level-convolutional-networks-for-text-classification.md)
- [Using the Output Embedding to Improve Language Models](./using-the-output-embedding-to-improve-language-models.md)
- Improving Neural Language Models with a Continuous Cache
- [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](./on-the-properties-of-neural-machine-translation-encoder-decoder-approaches.md)
- Full-Capacity Unitary Recurrent Neural Networks
- Hierarchical Recurrent Neural Networks for Long-Term Dependencies
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- [WaveNet - A Generative Model for Raw Audio](./wavenet-a-generative-model-for-raw-audio.md)
- Learning Precise Timing with LSTM Recurrent Networks
- [Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations](./zoneout-regularizing-rnns-by-randomly-preserving-hidden-activations.md)
- Breaking the Softmax Bottleneck - A High-Rank RNN Language Model
- Gated Word-Character Recurrent Language Model
- [On the difficulty of training recurrent neural networks](./on-the-difficulty-of-training-recurrent-neural-networks.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- The LAMBADA dataset - Word prediction requiring a broad discourse context
- Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
- Learning Character-level Representations for Part-of-Speech Tagging
- [Layer Normalization](./layer-normalization.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Parallel Networks that Learn to Pronounce English Text
- Connectionist Learning Procedures
- [Gradient-based learning applied to document recognition](./gradient-based-learning-applied-to-document-recognition.md)
- Finding Structure in Time
- Phoneme recognition using time-delay neural networks
- Speaker-independent isolated digit recognition - Multilayer perceptrons vs. Dynamic time warping
- Backpropagation Through Time - What It Does and How to Do It
- Backpropagation Applied to Handwritten Zip Code Recognition
- Building a Large Annotated Corpus of English - The Penn Treebank
- [Rectified Linear Units Improve Restricted Boltzmann Machines](./rectified-linear-units-improve-restricted-boltzmann-machines.md)
- Harmonising Chorales by Probabilistic Inference
- [Deep Learning](./deep-learning.md)
