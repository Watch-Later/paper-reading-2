---
title: BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

## References

- [MASS - Masked Sequence to Sequence Pre-training for Language Generation](./mass-masked-sequence-to-sequence-pre-training-for-language-generation.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](./unified-language-model-pre-training-for-natural-language-understanding-and-generation.md)
- Text Summarization with Pretrained Encoders
- [Attention is All you Need](./attention-is-all-you-need.md)
- Pre-trained language model representations for language generation
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](./exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer.md)
- [Get To The Point - Summarization with Pointer-Generator Networks](./get-to-the-point-summarization-with-pointer-generator-networks.md)
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- Leveraging Pre-trained Checkpoints for Sequence Generation Tasks
- [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](./albert-a-lite-bert-for-self-supervised-learning-of-language-representations.md)
- [Cross-lingual Language Model Pretraining](./cross-lingual-language-model-pretraining.md)
- [SpanBERT - Improving Pre-training by Representing and Predicting Spans](./spanbert-improving-pre-training-by-representing-and-predicting-spans.md)
- Edinburgh Neural Machine Translation Systems for WMT 16
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- Regularizing Neural Networks by Penalizing Confident Output Distributions
- The Second Conversational Intelligence Challenge (ConvAI2)
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](./a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference.md)
- Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization
- ELI5 - Long Form Question Answering
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- Controllable Abstractive Summarization
- [Efficient Estimation of Word Representations in Vector Space](./efficient-estimation-of-word-representations-in-vector-space.md)
- [Neural Network Acceptability Judgments](./neural-network-acceptability-judgments.md)
- Automatically Constructing a Corpus of Sentential Paraphrases
- The Winograd Schema Challenge
- [Gaussian Error Linear Units (GELUs)](./gaussian-error-linear-units-gelus.md)
- Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment
