---
title: Self-Attention with Relative Position Representations
authors:
- Peter Shaw
- Jakob Uszkoreit
- Ashish Vaswani
fieldsOfStudy:
- Computer Science
meta_key: self-attention-with-relative-position-representations
numCitedBy: 923
pdf_relpath: null
ref_count: 17
status: todo
tags:
- gen-from-ref
- paper
venue: NAACL
year: 2018
---

# Self-Attention with Relative Position Representations

## References

- [Effective Approaches to Attention-based Neural Machine Translation](./effective-approaches-to-attention-based-neural-machine-translation.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- [End-To-End Memory Networks](./end-to-end-memory-networks.md)
- [Graph Attention Networks](./graph-attention-networks.md)
- [Rethinking the Inception Architecture for Computer Vision](./rethinking-the-inception-architecture-for-computer-vision.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [Neural Machine Translation in Linear Time](./neural-machine-translation-in-linear-time.md)
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Convolutional Sequence to Sequence Learning](./convolutional-sequence-to-sequence-learning.md)
- [A Decomposable Attention Model for Natural Language Inference](./a-decomposable-attention-model-for-natural-language-inference.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- [Layer Normalization](./layer-normalization.md)
