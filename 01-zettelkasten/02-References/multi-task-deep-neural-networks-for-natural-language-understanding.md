---
title: Multi-Task Deep Neural Networks for Natural Language Understanding
authors:
- Xiaodong Liu
- Pengcheng He
- Weizhu Chen
- Jianfeng Gao
fieldsOfStudy:
- Computer Science
meta_key: multi-task-deep-neural-networks-for-natural-language-understanding
numCitedBy: 732
pdf_relpath: null
ref_count: 39
status: todo
tags:
- gen-from-ref
- paper
venue: ACL
year: 2019
---

# Multi-Task Deep Neural Networks for Natural Language Understanding

## References

- Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](./unified-language-model-pre-training-for-natural-language-understanding-and-generation.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Sentence Encoders on STILTs - Supplementary Training on Intermediate Labeled-data Tasks](./sentence-encoders-on-stilts-supplementary-training-on-intermediate-labeled-data-tasks.md)
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- Latent Multi-Task Architecture Learning
- [Attention is All you Need](./attention-is-all-you-need.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- [Multi-task Sequence to Sequence Learning](./multi-task-sequence-to-sequence-learning.md)
- The RepEval 2017 Shared Task - Multi-Genre Natural Language Inference with Sentence Representations
- Testing the Generalization Power of Neural Network Models across NLI Benchmarks
- Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval
- Multi-Task Learning for Machine Reading Comprehension
- Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation
- [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](./semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- A Survey on MultiTask Learning
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- Stochastic Answer Networks for Machine Reading Comprehension
- A Survey on Multi-Task Learning
- SciTaiL - A Textual Entailment Dataset from Science Question Answering
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- Stochastic Answer Networks for Natural Language Inference
- [Learning deep structured semantic models for web search using clickthrough data](./learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data.md)
- [Neural Network Acceptability Judgments](./neural-network-acceptability-judgments.md)
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- Breaking NLI Systems with Sentences that Require Simple Lexical Inferences
- Neural Approaches to Conversational AI
- Automatically Constructing a Corpus of Sentential Paraphrases
- The Winograd Schema Challenge
- Learning to rank using gradient descent
- Comparison of the predicted and observed secondary structure of T4 phage lysozyme.
- Multitask Learning
