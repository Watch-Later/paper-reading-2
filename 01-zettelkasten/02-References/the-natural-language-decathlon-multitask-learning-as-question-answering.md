---
title: The Natural Language Decathlon - Multitask Learning as Question Answering
authors:
- Bryan McCann
- N. Keskar
- Caiming Xiong
- R. Socher
fieldsOfStudy:
- Education
meta_key: the-natural-language-decathlon-multitask-learning-as-question-answering
numCitedBy: 410
pdf_relpath: null
ref_count: 138
status: todo
tags:
- gen-from-ref
- paper
venue: ArXiv
year: 2018
---

# The Natural Language Decathlon - Multitask Learning as Question Answering

## References

- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- Question Answering through Transfer Learning from Large Fine-grained Supervision Data
- Stochastic Answer Networks for Machine Reading Comprehension
- Contextualized Word Representations for Reading Comprehension
- [Pointing the Unknown Words](./pointing-the-unknown-words.md)
- MEMEN - Multi-layer Embedding with Memory Networks for Machine Comprehension
- [Machine Comprehension Using Match-LSTM and Answer Pointer](./machine-comprehension-using-match-lstm-and-answer-pointer.md)
- [NewsQA - A Machine Comprehension Dataset](./newsqa-a-machine-comprehension-dataset.md)
- [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](./ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
- Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization
- [TriviaQA - A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](./triviaqa-a-large-scale-distantly-supervised-challenge-dataset-for-reading-comprehension.md)
- Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation
- Question-Answer Driven Semantic Role Labeling - Using Natural Language to Annotate Natural Language
- [Natural Language Processing (Almost) from Scratch](./natural-language-processing-almost-from-scratch.md)
- On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](./a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference.md)
- Natural Language to Structured Query Generation via Meta-Learning
- [Bidirectional Attention Flow for Machine Comprehension](./bidirectional-attention-flow-for-machine-comprehension.md)
- DR-BiLSTM - Dependent Reading Bidirectional LSTM for Natural Language Inference
- Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations
- Neural Semantic Encoders
- [Dynamic Memory Networks for Visual and Textual Question Answering](./dynamic-memory-networks-for-visual-and-textual-question-answering.md)
- [Coarse-to-Fine Decoding for Neural Semantic Parsing](./coarse-to-fine-decoding-for-neural-semantic-parsing.md)
- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- Towards Improving Abstractive Summarization via Entailment Generation
- Making Neural QA as Simple as Possible but not Simpler
- [QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension](./qanet-combining-local-convolution-with-global-self-attention-for-reading-comprehension.md)
- Fine-tuned Language Models for Text Classification
- Sluice networks - Learning what to share between loosely related tasks
- [Skip-Thought Vectors](./skip-thought-vectors.md)
- Google's Multilingual Neural Machine Translation System - Enabling Zero-Shot Translation
- [A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks](./a-joint-many-task-model-growing-a-neural-network-for-multiple-nlp-tasks.md)
- Deep Semantic Role Labeling - What Works and What's Next
- A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference
- Deep Semantic Role Labeling with Self-Attention
- [A Deep Reinforced Model for Abstractive Summarization](./a-deep-reinforced-model-for-abstractive-summarization.md)
- FusionNet - Fusing via Fully-Aware Attention with Application to Machine Comprehension
- The Winograd Schema Challenge
- [Learned in Translation - Contextualized Word Vectors](./learned-in-translation-contextualized-word-vectors.md)
- Entailment, intensionality and text understanding
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](./abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond.md)
- DCN+ - Mixed Objective and Deep Residual Coattention for Question Answering
- Seq2SQL - Generating Structured Queries from Natural Language using Reinforcement Learning
- Reinforced Self-Attention Network - a Hybrid of Hard and Soft Attention for Sequence Modeling
- [Dynamic Coattention Networks For Question Answering](./dynamic-coattention-networks-for-question-answering.md)
- Zero-Shot Relation Extraction via Reading Comprehension
- [Get To The Point - Summarization with Pointer-Generator Networks](./get-to-the-point-summarization-with-pointer-generator-networks.md)
- [Multi-task Sequence to Sequence Learning](./multi-task-sequence-to-sequence-learning.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [Effective Approaches to Attention-based Neural Machine Translation](./effective-approaches-to-attention-based-neural-machine-translation.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- Learning to Compose Task-Specific Tree Structures
- Reinforced Mnemonic Reader for Machine Reading Comprehension
- End-to-end learning of semantic role labeling using recurrent neural networks
- [Bleu - a Method for Automatic Evaluation of Machine Translation](./bleu-a-method-for-automatic-evaluation-of-machine-translation.md)
- Neural Belief Tracker - Data-Driven Dialogue State Tracking
- One Model To Learn Them All
- [Pointer Sentinel Mixture Models](./pointer-sentinel-mixture-models.md)
- A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling
- [Unsupervised Pretraining for Sequence to Sequence Learning](./unsupervised-pretraining-for-sequence-to-sequence-learning.md)
- Lifelong Learning Algorithms
- [Learning to Generate Reviews and Discovering Sentiment](./learning-to-generate-reviews-and-discovering-sentiment.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- Transfer Learning for Low-Resource Neural Machine Translation
- [Gated Self-Matching Networks for Reading Comprehension and Question Answering](./gated-self-matching-networks-for-reading-comprehension-and-question-answering.md)
- Phase Conductor on Multi-layered Attentions for Machine Comprehension
- [Attention is All you Need](./attention-is-all-you-need.md)
- A Network-based End-to-End Trainable Task-oriented Dialogue System
- Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information
- Multi-Reward Reinforced Summarization with Saliency and Entailment
- The IWSLT 2016 Evaluation Campaign
- The Importance of Syntactic Parsing and Inference in Semantic Role Labeling
- Recognising Textual Entailment with Robust Logical Inference
- The PASCAL Recognising Textual Entailment Challenge
- [Efficient Estimation of Word Representations in Vector Space](./efficient-estimation-of-word-representations-in-vector-space.md)
- [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](./improved-semantic-representations-from-tree-structured-long-short-term-memory-networks.md)
- Neural Tree Indexers for Text Understanding
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- Measuring Catastrophic Forgetting in Neural Networks
- Meta-Learning with Memory-Augmented Neural Networks
- Bilateral Multi-Perspective Matching for Natural Language Sentences
- Distance-based Self-Attention Network for Natural Language Inference
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- TypeSQL - Knowledge-Based Type-Aware Neural Text-to-SQL Generation
- The University of Edinburgh's Neural MT Systems for WMT17
- Overcoming catastrophic forgetting in neural networks
- The Best of Both Worlds - Combining Recent Advances in Neural Machine Translation
- PackNet - Adding Multiple Tasks to a Single Network by Iterative Pruning
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- Overcoming Catastrophic Forgetting by Incremental Moment Matching
- Understanding natural language
- From Group to Individual Labels Using Deep Features
- [Incorporating Copying Mechanism in Sequence-to-Sequence Learning](./incorporating-copying-mechanism-in-sequence-to-sequence-learning.md)
- An extended model of natural logic
- Catastrophic Forgetting, Rehearsal and Pseudorehearsal
- Dependency-based Semantic Role Labeling of PropBank
- Newsroom - A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies
- The Need for Biases in Learning Generalizations
- Multitask Learning
- [Pointer Networks](./pointer-networks.md)
- A Natural Logic Inference System
- [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](./framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures.md)
- A Perspective View and Survey of Meta-Learning
- [Convolutional Sequence to Sequence Learning](./convolutional-sequence-to-sequence-learning.md)
- Learning to Learn - Introduction and Overview
- [Curriculum learning](./curriculum-learning.md)
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](./model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks.md)
- Catastrophic Interference in Connectionist Networks - The Sequential Learning Problem
- A Bio-Inspired Incremental Learning Architecture for Applied Perceptual Problems
- Connectionist models of recognition memory - constraints imposed by learning and forgetting functions.
- [ROUGE - A Package for Automatic Evaluation of Summaries](./rouge-a-package-for-automatic-evaluation-of-summaries.md)
- Global-Locally Self-Attentive Dialogue State Tracker
- PathNet - Evolution Channels Gradient Descent in Super Neural Networks
- [Long Short-Term Memory](./long-short-term-memory.md)
- Learning to Learn Using Gradient Descent
- Learning to Control Fast-Weight Memories - An Alternative to Dynamic Recurrent Networks
- Learning to learn by gradient descent by gradient descent
- [Layer Normalization](./layer-normalization.md)
- [Rectified Linear Units Improve Restricted Boltzmann Machines](./rectified-linear-units-improve-restricted-boltzmann-machines.md)
- On the Optimization of a Synaptic Learning Rule
- Multi-objective Optimization
- End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking
- Natural Language Inference with External Knowledge
