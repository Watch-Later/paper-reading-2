---
title: End-to-end attention-based large vocabulary speech recognition
authors:
- Dzmitry Bahdanau
- J. Chorowski
- Dmitriy Serdyuk
- Philemon Brakel
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: end-to-end-attention-based-large-vocabulary-speech-recognition
numCitedBy: 900
pdf_relpath: null
ref_count: 35
status: todo
tags:
- gen-from-ref
- paper
venue: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)
year: 2016
---

# End-to-end attention-based large vocabulary speech recognition

## References

- EESEN - End-to-end speech recognition using deep RNN models and WFST-based decoding
- [Speech recognition with deep recurrent neural networks](./speech-recognition-with-deep-recurrent-neural-networks.md)
- End-to-end Continuous Speech Recognition using Attention-based Recurrent NN - First Results
- [Attention-Based Models for Speech Recognition](./attention-based-models-for-speech-recognition.md)
- First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs
- [Towards End-To-End Speech Recognition with Recurrent Neural Networks](./towards-end-to-end-speech-recognition-with-recurrent-neural-networks.md)
- Listen, Attend and Spell
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [Deep Speech - Scaling up end-to-end speech recognition](./deep-speech-scaling-up-end-to-end-speech-recognition.md)
- [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](./deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups.md)
- Review of Neural Networks for Speech Recognition
- Sequence Transduction with Recurrent Neural Networks
- Weighted finite-state transducers in speech recognition
- [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](./connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks.md)
- THE USE OF RECURRENT NEURAL NETWORKS IN CONTINUOUS SPEECH RECOGNITION
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- On Using Monolingual Corpora in Neural Machine Translation
- A Clockwork RNN
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Gated Feedback Recurrent Neural Networks
- [Improving neural networks by preventing co-adaptation of feature detectors](./improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
- High-dimensional sequence transduction
- [Recurrent Models of Visual Attention](./recurrent-models-of-visual-attention.md)
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](./show-attend-and-tell-neural-image-caption-generation-with-visual-attention.md)
- OpenFst - A General and Efficient Weighted Finite-State Transducer Library
- [ADADELTA - An Adaptive Learning Rate Method](./adadelta-an-adaptive-learning-rate-method.md)
- [Theano - new features and speed improvements](./theano-new-features-and-speed-improvements.md)
- Blocks and Fuel - Frameworks for deep learning
