---
title: Unified Vision-Language Pre-Training for Image Captioning and VQA
authors:
- Luowei Zhou
- H. Palangi
- Lei Zhang
- Houdong Hu
- Jason J. Corso
- Jianfeng Gao
fieldsOfStudy:
- Computer Science
meta_key: unified-vision-language-pre-training-for-image-captioning-and-vqa
numCitedBy: 355
pdf_relpath: null
ref_count: 48
status: todo
tags:
- gen-from-ref
- paper
venue: AAAI
year: 2020
---

# Unified Vision-Language Pre-Training for Image Captioning and VQA

## References

- [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](./unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training.md)
- [UNITER - UNiversal Image-TExt Representation Learning](./uniter-universal-image-text-representation-learning.md)
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](./unified-language-model-pre-training-for-natural-language-understanding-and-generation.md)
- [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](./vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.md)
- [UNITER - Learning UNiversal Image-TExt Representations](./uniter-learning-universal-image-text-representations.md)
- [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](./lxmert-learning-cross-modality-encoder-representations-from-transformers.md)
- [VideoBERT - A Joint Model for Video and Language Representation Learning](./videobert-a-joint-model-for-video-and-language-representation-learning.md)
- [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](./vl-bert-pre-training-of-generic-visual-linguistic-representations.md)
- [VisualBERT - A Simple and Performant Baseline for Vision and Language](./visualbert-a-simple-and-performant-baseline-for-vision-and-language.md)
- [Neural Baby Talk](./neural-baby-talk.md)
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- [Multi-Task Deep Neural Networks for Natural Language Understanding](./multi-task-deep-neural-networks-for-natural-language-understanding.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- [Exploring Visual Relationship for Image Captioning](./exploring-visual-relationship-for-image-captioning.md)
- [End-to-End Dense Video Captioning with Masked Transformer](./end-to-end-dense-video-captioning-with-masked-transformer.md)
- [Attention on Attention for Image Captioning](./attention-on-attention-for-image-captioning.md)
- [Yin and Yang - Balancing and Answering Binary Visual Questions](./yin-and-yang-balancing-and-answering-binary-visual-questions.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](./bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.md)
- [Contrastive Bidirectional Transformer for Temporal Representation Learning](./contrastive-bidirectional-transformer-for-temporal-representation-learning.md)
- [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](./making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering.md)
- [Fusion of Detected Objects in Text for Visual Question Answering](./fusion-of-detected-objects-in-text-for-visual-question-answering.md)
- [Auto-Encoding Scene Graphs for Image Captioning](./auto-encoding-scene-graphs-for-image-captioning.md)
- [VQA - Visual Question Answering](./vqa-visual-question-answering.md)
- Grounded Video Description
- [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](./visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations.md)
- [Learning Video Representations using Contrastive Bidirectional Transformer](./learning-video-representations-using-contrastive-bidirectional-transformer.md)
- [Don't Just Assume; Look and Answer - Overcoming Priors for Visual Question Answering](./don-t-just-assume-look-and-answer-overcoming-priors-for-visual-question-answering.md)
- [Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering](./dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering.md)
- [Self-Critical Sequence Training for Image Captioning](./self-critical-sequence-training-for-image-captioning.md)
- [Aggregated Residual Transformations for Deep Neural Networks](./aggregated-residual-transformations-for-deep-neural-networks.md)
- [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](./conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning.md)
- [Pythia v0.1 - the Winning Entry to the VQA Challenge 2018](./pythia-v0-1-the-winning-entry-to-the-vqa-challenge-2018.md)
- [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](./faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.md)
- [Bilinear Attention Networks](./bilinear-attention-networks.md)
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- [Microsoft COCO Captions - Data Collection and Evaluation Server](./microsoft-coco-captions-data-collection-and-evaluation-server.md)
- [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](./from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions.md)
- Generative Question Answering - Learning to Answer the Whole Question
