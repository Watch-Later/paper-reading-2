---
title: Character-Level Language Modeling with Deeper Self-Attention
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Character-Level Language Modeling with Deeper Self-Attention

## References

- [Language Modeling with Gated Convolutional Networks](./language-modeling-with-gated-convolutional-networks.md)
- Regularizing and Optimizing LSTM Language Models
- Multiplicative LSTM for sequence modelling
- SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
- [End-To-End Memory Networks](./end-to-end-memory-networks.md)
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- [LSTM Neural Networks for Language Modeling](./lstm-neural-networks-for-language-modeling.md)
- MuFuRU - The Multi-Function Recurrent Unit
- Memory Architectures in Recurrent Neural Network Language Models
- [Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations](./zoneout-regularizing-rnns-by-randomly-preserving-hidden-activations.md)
- [Attention is All you Need](./attention-is-all-you-need.md)
- Very Deep Convolutional Networks for Natural Language Processing
- Recurrent Highway Networks
- Fast-Slow Recurrent Neural Networks
- Dynamic Evaluation of Neural Sequence Models
- Sharp Nearby, Fuzzy Far Away - How Neural Language Models Use Context
- [Recurrent Batch Normalization](./recurrent-batch-normalization.md)
- One billion word benchmark for measuring progress in statistical language modeling
- Independently Recurrent Neural Network (IndRNN) - Building A Longer and Deeper RNN
- Frustratingly Short Attention Spans in Neural Language Modeling
- [Learning to Generate Reviews and Discovering Sentiment](./learning-to-generate-reviews-and-discovering-sentiment.md)
- [Character-level Convolutional Networks for Text Classification](./character-level-convolutional-networks-for-text-classification.md)
- [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](./outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer.md)
- Neural Architecture Search with Reinforcement Learning
- [Neural Machine Translation in Linear Time](./neural-machine-translation-in-linear-time.md)
- Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
- Surprisal-Driven Feedback in Recurrent Networks
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- Gated Feedback Recurrent Neural Networks
- [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](./a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
- [Recurrent Neural Network Regularization](./recurrent-neural-network-regularization.md)
- [Extensions of recurrent neural network language model](./extensions-of-recurrent-neural-network-language-model.md)
- [Memory Networks](./memory-networks.md)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- Improving Neural Language Models with a Continuous Cache
- [On Multiplicative Integration with Recurrent Neural Networks](./on-multiplicative-integration-with-recurrent-neural-networks.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](./batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
- Sparse Attentive Backtracking - Long-Range Credit Assignment in Recurrent Networks
- Unitary Evolution Recurrent Neural Networks
- Byte-Level Machine Reading Across Morphologically Varied Languages
- [Hierarchical Multiscale Recurrent Neural Networks](./hierarchical-multiscale-recurrent-neural-networks.md)
- [Layer Normalization](./layer-normalization.md)
- A Neural Probabilistic Language Model
- Architectural Complexity Measures of Recurrent Neural Networks
- Unbiasing Truncated Backpropagation Through Time
- [Improving GANs Using Optimal Transport](./improving-gans-using-optimal-transport.md)
- Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
- Backpropagation Through Time - What It Does and How to Do It
