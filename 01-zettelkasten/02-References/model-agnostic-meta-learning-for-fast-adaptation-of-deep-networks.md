---
title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

## References

- Optimization as a Model for Few-Shot Learning
- Meta-SGD - Learning to Learn Quickly for Few Shot Learning
- [Actor-Mimic - Deep Multitask and Transfer Reinforcement Learning](./actor-mimic-deep-multitask-and-transfer-reinforcement-learning.md)
- Learning to reinforcement learn
- On First-Order Meta-Learning Algorithms
- Overcoming catastrophic forgetting in neural networks
- How to train your MAML
- Meta-Learning with Memory-Augmented Neural Networks
- [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](./batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
- [Matching Networks for One Shot Learning](./matching-networks-for-one-shot-learning.md)
- [Data-dependent Initializations of Convolutional Neural Networks](./data-dependent-initializations-of-convolutional-neural-networks.md)
- Benchmarking Deep Reinforcement Learning for Continuous Control
- Siamese Neural Networks for One-Shot Image Recognition
- One-shot Learning with Memory-Augmented Neural Networks
- Meta Networks
- RL$^2$ - Fast Reinforcement Learning via Slow Reinforcement Learning
- Learning to Learn Using Gradient Descent
- [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](./weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md)
- One-Shot Generalization in Deep Generative Models
- Prototypical Networks for Few-shot Learning
- Towards a Neural Statistician
- Meta-neural networks that learn by learning
- Learning to learn by gradient descent by gradient descent
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- Learning a synaptic learning rule
- [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](./decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition.md)
- Learning to Remember Rare Events
- Fast learning for problem classes using knowledge based network initialization
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](./exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.md)
- On the Optimization of a Synaptic Learning Rule
- Gradient-based Hyperparameter Optimization through Reversible Learning
- [Simple statistical gradient-following algorithms for connectionist reinforcement learning](./simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning.md)
- Learning to Control Fast-Weight Memories - An Alternative to Dynamic Recurrent Networks
- [Explaining and Harnessing Adversarial Examples](./explaining-and-harnessing-adversarial-examples.md)
- Attentive Recurrent Comparators
- One shot learning of simple visual concepts
- Using Fast Weights to Attend to the Recent Past
- [TensorFlow - Large-Scale Machine Learning on Heterogeneous Distributed Systems](./tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems.md)
- Online Representation Learning in Recurrent Neural Language Models
- Learning to Optimize âˆ—
- Trust Region Policy Optimization
- Using fast weights to deblur old memories
- One-Shot Video Object Segmentation
- Learning to Learn
- Evolvability Search - Directly Selecting for Evolvability in order to Study and Produce It
- MuJoCo - A physics engine for model-based control
- Model-Agnostic Meta-Learning for Multimodal Task Distributions
- International Conference on Learning Representations (ICLR)
