---
title: Training Products of Experts by Minimizing Contrastive Divergence
authors:
- Geoffrey E. Hinton
fieldsOfStudy:
- Computer Science
meta_key: training-products-of-experts-by-minimizing-contrastive-divergence
numCitedBy: 4571
pdf_relpath: null
ref_count: 34
status: todo
tags:
- gen-from-ref
- paper
venue: Neural Computation
year: 2002
---

# Training Products of Experts by Minimizing Contrastive Divergence

## References

- Recognizing Hand-written Digits Using Hierarchical Products of Experts
- Connectionist Learning of Belief Networks
- Rate-coded Restricted Boltzmann Machines for Face Recognition
- Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
- Using Generative Models for Handwritten Digit Recognition
- A Gradient-Based Boosting Algorithm for Regression Problems
- Products of Hidden Markov Models
- MAXIMUM ENTROPY
- Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images
- Unsupervised learning of distributions
- Combining Probability Distributions - A Critique and an Annotated Bibliography
- Biologically Plausible Error-Driven Learning Using Local Activation Differences - The Generalized Recirculation Algorithm
- Learning Representations by Recirculation
- Learning Continuous Attractors in Recurrent Networks
- Bias/Variance Decompositions for Likelihood-Based Estimators
- Information processing in dynamical systems - foundations of harmony theory
- Attractor Dynamics in Feedforward Neural Networks
- A Maximum Entropy Approach to Natural Language Processing
- Mean Field Theory for Sigmoid Belief Networks
- Learning and relearning in Boltzmann machines
- The wake-sleep algorithm for unsupervised neural networks.
- Learning Structural Descriptions From Examples
- The psychology of computer vision
