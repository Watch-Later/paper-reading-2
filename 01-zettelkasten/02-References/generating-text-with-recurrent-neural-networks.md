---
title: Generating Text with Recurrent Neural Networks
authors:
- Ilya Sutskever
- James Martens
- Geoffrey E. Hinton
fieldsOfStudy:
- Computer Science
meta_key: generating-text-with-recurrent-neural-networks
numCitedBy: 1254
pdf_relpath: null
ref_count: 30
status: todo
tags:
- gen-from-ref
- paper
venue: ICML
year: 2011
---

# Generating Text with Recurrent Neural Networks

## References

- Learning Recurrent Neural Networks with Hessian-Free Optimization
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- Learning long-term dependencies with gradient descent is difficult
- A Scalable Hierarchical Distributed Language Model
- [Long Short-Term Memory](./long-short-term-memory.md)
- Learning representations by back-propagating errors
- Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
- Factored conditional restricted Boltzmann Machines for modeling motion style
- A stochastic memoizer for sequence data
- Backpropagation Through Time - What It Does and How to Do It
- An application of recurrent nets to phone probability estimation
- Deep learning via Hessian-free optimization
- Learning representations by backpropagating errors
- Dynamic bayesian networks - representation, inference and learning
- Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles
- Adaptive weighing of context models for lossless data compression
- The BellKor solution to the Netflix Prize
- Lossless Compression Based on the Sequence Memoizer
- CUDAMat - a CUDA-based matrix class for Python
- Harnessing Nonlinearity - Predicting Chaotic Systems and Saving Energy in Wireless Communication
- Observable Operator Models for Discrete Stochastic Time Series
- Dasher-a data entry interface using continuous gestures and language models
- Arithmetic Coding
- Untersuchungen zu dynamischen neuronalen Netzen
