---
title: Linguistic Regularities in Continuous Space Word Representations
authors:
- Tomas Mikolov
- Wen-tau Yih
- G. Zweig
fieldsOfStudy:
- Computer Science
meta_key: linguistic-regularities-in-continuous-space-word-representations
numCitedBy: 3051
pdf_relpath: null
ref_count: 24
status: todo
tags:
- gen-from-ref
- paper
venue: NAACL
year: 2013
---

# Linguistic Regularities in Continuous Space Word Representations

## References

- A Neural Probabilistic Language Model
- Neural Probabilistic Language Models
- [Efficient Estimation of Word Representations in Vector Space](./efficient-estimation-of-word-representations-in-vector-space.md)
- Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing
- Continuous space language models
- Word Representations - A Simple and General Method for Semi-Supervised Learning
- UTD - Determining Relational Similarity Using Lexical Patterns
- Distributed representations, simple recurrent networks, and grammatical structure
- Discovering Binary Codes for Documents by Learning Deep Generative Models
- Structured Output Layer neural network language model
- Hierarchical Probabilistic Neural Network Language Model
- [A unified architecture for natural language processing - deep neural networks with multitask learning](./a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning.md)
- Domain and Function - A Dual-Space Model of Semantic Relations and Compositions
- A Scalable Hierarchical Distributed Language Model
- Recursive Distributed Representations
- SemEval-2012 Task 2 - Measuring Degrees of Relational Similarity
- Building a Large Annotated Corpus of English - The Penn Treebank
- Strategies for training large scale neural network language models
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- Indexing by Latent Semantic Analysis
- [Reducing the Dimensionality of Data with Neural Networks](./reducing-the-dimensionality-of-data-with-neural-networks.md)
- Learning distributed representations of concepts.
