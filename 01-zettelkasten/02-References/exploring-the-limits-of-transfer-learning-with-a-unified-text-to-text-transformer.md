---
title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
pdf_relpath: null
status: todo
tags:
- gen-from-ref
- paper
---

# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

## References

- [Universal Language Model Fine-tuning for Text Classification](./universal-language-model-fine-tuning-for-text-classification.md)
- [StructBERT - Incorporating Language Structures into Pre-training for Deep Language Understanding](./structbert-incorporating-language-structures-into-pre-training-for-deep-language-understanding.md)
- Transfer Learning in Natural Language Processing
- [Language Models are Unsupervised Multitask Learners](./language-models-are-unsupervised-multitask-learners.md)
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](./unified-language-model-pre-training-for-natural-language-understanding-and-generation.md)
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- [Multi-Task Deep Neural Networks for Natural Language Understanding](./multi-task-deep-neural-networks-for-natural-language-understanding.md)
- [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](./glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding.md)
- [Unsupervised Pretraining for Sequence to Sequence Learning](./unsupervised-pretraining-for-sequence-to-sequence-learning.md)
- Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning
- [SciBERT - A Pretrained Language Model for Scientific Text](./scibert-a-pretrained-language-model-for-scientific-text.md)
- [MASS - Masked Sequence to Sequence Pre-training for Language Generation](./mass-masked-sequence-to-sequence-pre-training-for-language-generation.md)
- [A Simple Method for Commonsense Reasoning](./a-simple-method-for-commonsense-reasoning.md)
- [DistilBERT, a distilled version of BERT - smaller, faster, cheaper and lighter](./distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter.md)
- Parameter-Efficient Transfer Learning for NLP
- [Sentence Encoders on STILTs - Supplementary Training on Intermediate Labeled-data Tasks](./sentence-encoders-on-stilts-supplementary-training-on-intermediate-labeled-data-tasks.md)
- [Cross-lingual Language Model Pretraining](./cross-lingual-language-model-pretraining.md)
- TinyBERT - Distilling BERT for Natural Language Understanding
- [Sequence to Sequence Learning with Neural Networks](./sequence-to-sequence-learning-with-neural-networks.md)
- [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](./supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data.md)
- Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling
- [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding.md)
- The Bottom-up Evolution of Representations in the Transformer - A Study with Machine Translation and Language Modeling Objectives
- Simple, Scalable Adaptation for Neural Machine Translation
- [Get To The Point - Summarization with Pointer-Generator Networks](./get-to-the-point-summarization-with-pointer-generator-networks.md)
- BoolQ - Exploring the Surprising Difficulty of Natural Yes/No Questions
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- SummAE - Zero-Shot Abstractive Text Summarization using Length-Agnostic Auto-Encoders
- [A Deep Reinforced Model for Abstractive Summarization](./a-deep-reinforced-model-for-abstractive-summarization.md)
- [Skip-Thought Vectors](./skip-thought-vectors.md)
- [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](./abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond.md)
- Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges
- Unifying Question Answering and Text Classification via Span Extraction
- [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](./albert-a-lite-bert-for-self-supervised-learning-of-language-representations.md)
- [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](./semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation.md)
- Understanding Back-Translation at Scale
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- Semi-supervised Sequence Learning
- [QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension](./qanet-combining-local-convolution-with-global-self-attention-for-reading-comprehension.md)
- Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval
- To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- CTRL - A Conditional Transformer Language Model for Controllable Generation
- [Attention is All you Need](./attention-is-all-you-need.md)
- [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](./google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
- Snorkel MeTaL - Weak Supervision for Multi-Task Learning
- WiC - 10, 000 Example Pairs for Evaluating Context-Sensitive Representations
- [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./xlnet-generalized-autoregressive-pretraining-for-language-understanding.md)
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](./a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference.md)
- [A Convolutional Neural Network for Modelling Sentences](./a-convolutional-neural-network-for-modelling-sentences.md)
- Exploring the Limits of Weakly Supervised Pretraining
- [An efficient framework for learning sentence representations](./an-efficient-framework-for-learning-sentence-representations.md)
- [Learning Distributed Representations of Sentences from Unlabelled Data](./learning-distributed-representations-of-sentences-from-unlabelled-data.md)
- SentencePiece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing
- Looking Beyond the Surface - A Challenge Set for Reading Comprehension over Multiple Sentences
- Learning Word Vectors for 157 Languages
- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- Cloze-driven Pretraining of Self-attention Networks
- [SQuAD - 100,000+ Questions for Machine Comprehension of Text](./squad-100-000-questions-for-machine-comprehension-of-text.md)
- [TriviaQA - A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](./triviaqa-a-large-scale-distantly-supervised-challenge-dataset-for-reading-comprehension.md)
- [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](./outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer.md)
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Neural Machine Translation of Rare Words with Subword Units](./neural-machine-translation-of-rare-words-with-subword-units.md)
- [SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems](./superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.md)
- [Character-Level Language Modeling with Deeper Self-Attention](./character-level-language-modeling-with-deeper-self-attention.md)
- Fine-tune BERT for Extractive Summarization
- A Call for Clarity in Reporting BLEU Scores
- N-gram Counts and Language Models from the Common Crawl
- FreeLB - Enhanced Adversarial Training for Language Understanding
- [NewsQA - A Machine Comprehension Dataset](./newsqa-a-machine-comprehension-dataset.md)
- [Subword Regularization - Improving Neural Network Translation Models with Multiple Subword Candidates](./subword-regularization-improving-neural-network-translation-models-with-multiple-subword-candidates.md)
- A Surprisingly Robust Trick for the Winograd Schema Challenge
- [SpanBERT - Improving Pre-training by Representing and Predicting Spans](./spanbert-improving-pre-training-by-representing-and-predicting-spans.md)
- [Self-Attention with Relative Position Representations](./self-attention-with-relative-position-representations.md)
- Generating Sentences from a Continuous Space
- C4Corpus - Multilingual Web-size Corpus with Free License
- [SentEval - An Evaluation Toolkit for Universal Sentence Representations](./senteval-an-evaluation-toolkit-for-universal-sentence-representations.md)
- Deep Learning Scaling is Predictable, Empirically
- [Long Short-Term Memory-Networks for Machine Reading](./long-short-term-memory-networks-for-machine-reading.md)
- [Distilling the Knowledge in a Neural Network](./distilling-the-knowledge-in-a-neural-network.md)
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- Findings of the 2015 Workshop on Statistical Machine Translation
- A Hybrid Neural Network Model for Commonsense Reasoning
- [An Overview of Multi-Task Learning in Deep Neural Networks](./an-overview-of-multi-task-learning-in-deep-neural-networks.md)
- What makes ImageNet good for transfer learning?
- Findings of the 2016 Conference on Machine Translation
- Generating Wikipedia by Summarizing Long Sequences
- Dirt Cheap Web-Scale Parallel Text from the Common Crawl
- [Rethinking ImageNet Pre-Training](./rethinking-imagenet-pre-training.md)
- [Deep Residual Learning for Image Recognition](./deep-residual-learning-for-image-recognition.md)
- [Distributed Representations of Words and Phrases and their Compositionality](./distributed-representations-of-words-and-phrases-and-their-compositionality.md)
- Do Better ImageNet Models Transfer Better?
- Findings of the 2014 Workshop on Statistical Machine Translation
- [An Overview of Multi-task Learning](./an-overview-of-multi-task-learning.md)
- [Dropout - a simple way to prevent neural networks from overfitting](./dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
- [Efficient Estimation of Word Representations in Vector Space](./efficient-estimation-of-word-representations-in-vector-space.md)
- [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](./aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books.md)
- Automatically Constructing a Corpus of Sentential Paraphrases
- Defending Against Neural Fake News
- Multitask Learning
- Resolving Complex Cases of Definite Pronouns - The Winograd Schema Challenge
- [Neural Network Acceptability Judgments](./neural-network-acceptability-judgments.md)
- [How transferable are features in deep neural networks?](./how-transferable-are-features-in-deep-neural-networks.md)
- Music Transformer - Generating Music with Long-Term Structure
- Adafactor - Adaptive Learning Rates with Sublinear Memory Cost
- The PASCAL Recognising Textual Entailment Challenge
- [ROUGE - A Package for Automatic Evaluation of Summaries](./rouge-a-package-for-automatic-evaluation-of-summaries.md)
- [ImageNet - A large-scale hierarchical image database](./imagenet-a-large-scale-hierarchical-image-database.md)
- [Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks](./learning-and-transferring-mid-level-image-representations-using-convolutional-neural-networks.md)
- [Bleu - a Method for Automatic Evaluation of Machine Translation](./bleu-a-method-for-automatic-evaluation-of-machine-translation.md)
- Measuring the Effects of Data Parallelism on Neural Network Training
- [Mesh-TensorFlow - Deep Learning for Supercomputers](./mesh-tensorflow-deep-learning-for-supercomputers.md)
- [Caffe - Convolutional Architecture for Fast Feature Embedding](./caffe-convolutional-architecture-for-fast-feature-embedding.md)
- Memory-Efficient Adaptive Optimization for Large-Scale Learning
- ReCoRD - Bridging the Gap between Human and Machine Commonsense Reading Comprehension
- GPipe - Efficient Training of Giant Neural Networks using Pipeline Parallelism
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- Federated Learning - Strategies for Improving Communication Efficiency
- The CommitmentBank - Investigating projection in naturally occurring discourse
- The Winograd Schema Challenge
- [The Natural Language Decathlon - Multitask Learning as Question Answering](./the-natural-language-decathlon-multitask-learning-as-question-answering.md)
- [ImageNet Large Scale Visual Recognition Challenge](./imagenet-large-scale-visual-recognition-challenge.md)
- Federated Optimization - Distributed Optimization Beyond the Datacenter
- SemEval-2012 Task 7 - Choice of Plausible Alternatives - An Evaluation of Commonsense Causal Reasoning
- A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
- One weird trick for parallelizing convolutional neural networks
- “Cloze Procedure” - A New Tool for Measuring Readability
- [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./roberta-a-robustly-optimized-bert-pretraining-approach.md)
- [ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators](./electra-pre-training-text-encoders-as-discriminators-rather-than-generators.md)
- Neural transfer learning for natural language processing
- Low-field SQUID MRI - To tune or not to tune?
- A bitter lesson.
