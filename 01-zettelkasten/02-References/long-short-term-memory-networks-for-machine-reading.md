---
title: Long Short-Term Memory-Networks for Machine Reading
authors:
- Jianpeng Cheng
- Li Dong
- Mirella Lapata
fieldsOfStudy:
- Computer Science
meta_key: long-short-term-memory-networks-for-machine-reading
numCitedBy: 765
pdf_relpath: null
ref_count: 94
status: todo
tags:
- gen-from-ref
- paper
venue: EMNLP
year: 2016
---

# Long Short-Term Memory-Networks for Machine Reading

## References

- [Memory Networks](./memory-networks.md)
- [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](./ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
- Recurrent Memory Networks for Language Modeling
- Recurrent Memory Network for Language Modeling
- [End-To-End Memory Networks](./end-to-end-memory-networks.md)
- [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](./improved-semantic-representations-from-tree-structured-long-short-term-memory-networks.md)
- [A Convolutional Neural Network for Modelling Sentences](./a-convolutional-neural-network-for-modelling-sentences.md)
- [Dynamic Memory Networks for Visual and Textual Question Answering](./dynamic-memory-networks-for-visual-and-textual-question-answering.md)
- Depth-Gated LSTM
- A Deep Memory-based Architecture for Sequence-to-Sequence Learning
- [Grid Long Short-Term Memory](./grid-long-short-term-memory.md)
- Depth-Gated Recurrent Neural Networks
- Deep Recursive Neural Networks for Compositionality in Language
- [Learning Natural Language Inference with LSTM](./learning-natural-language-inference-with-lstm.md)
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- [Teaching Machines to Read and Comprehend](./teaching-machines-to-read-and-comprehend.md)
- Learning to Execute
- [A Fast Unified Model for Parsing and Sentence Understanding](./a-fast-unified-model-for-parsing-and-sentence-understanding.md)
- Learning Longer Memory in Recurrent Neural Networks
- A Clockwork RNN
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](./learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
- Learning Context-free Grammars - Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory
- [Long Short-Term Memory](./long-short-term-memory.md)
- [Learning to Compose Neural Networks for Question Answering](./learning-to-compose-neural-networks-for-question-answering.md)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](./recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
- [Reasoning about Entailment with Neural Attention](./reasoning-about-entailment-with-neural-attention.md)
- [A large annotated corpus for learning natural language inference](./a-large-annotated-corpus-for-learning-natural-language-inference.md)
- Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
- Learning to Transduce with Unbounded Memory
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Convolutional Neural Networks for Sentence Classification](./convolutional-neural-networks-for-sentence-classification.md)
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- [A Neural Attention Model for Abstractive Sentence Summarization](./a-neural-attention-model-for-abstractive-sentence-summarization.md)
- Learning Structured Embeddings of Knowledge Bases
- Learning long-term dependencies with gradient descent is difficult
- Molding CNNs for text - non-linear, non-consecutive convolutions
- [GloVe - Global Vectors for Word Representation](./glove-global-vectors-for-word-representation.md)
- Gated Feedback Recurrent Neural Networks
- Toward an Architecture for Never-Ending Language Learning
- Machine Reading - A Killer App for Statistical Relational AI
- [Distributed Representations of Sentences and Documents](./distributed-representations-of-sentences-and-documents.md)
- Insensitivity of the Human Sentence-Processing System to Hierarchical Structure
- Transition-Based Dependency Parsing with Stack Long Short-Term Memory
- Machine Reading
- DeepDive - Web-scale Knowledge-base Construction using Statistical Learning and Inference
- Locality and Parsing Complexity
- Open Information Extraction - The Second Generation
- Recovery from misanalyses of garden-path sentences â˜†
- Learning Context-free Grammars
- Unsupervised Ontology Induction from Text
- A computational model of human parsing
- [Identifying Relations for Open Information Extraction](./identifying-relations-for-open-information-extraction.md)
- Corpus-Based Induction of Syntactic Structure - Models of Dependency and Constituency
- [On the difficulty of training recurrent neural networks](./on-the-difficulty-of-training-recurrent-neural-networks.md)
- Injecting Logical Background Knowledge into Embeddings for Relation Extraction
- Integration of visual and linguistic information in spoken language comprehension.
- On the Computational Power of Neural Nets
- [Adam - A Method for Stochastic Optimization](./adam-a-method-for-stochastic-optimization.md)
- Filling Knowledge Gaps in Text for Machine Reading
- Knowledge Base Population - Successful Approaches and Challenges
- Extracting and evaluating general world knowledge from the Brown Corpus
- A study of the knowledge base requirements for passing an elementary science test
- The PASCAL Recognising Textual Entailment Challenge
- Eye movements in reading and information processing - 20 years of research.
- Untersuchungen zu dynamischen neuronalen Netzen
