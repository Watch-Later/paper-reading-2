---
title: Transformer-XL - Attentive Language Models beyond a Fixed-Length Context
authors:
- Zihang Dai
- Zhilin Yang
- Yiming Yang
- J. Carbonell
- Quoc V. Le
- R. Salakhutdinov
fieldsOfStudy:
- Computer Science
meta_key: transformer-xl-attentive-language-models-beyond-a-fixed-length-context
numCitedBy: 1771
pdf_relpath: null
ref_count: 68
status: todo
tags:
- gen-from-ref
- paper
venue: ACL
year: 2019
---

# Transformer-XL - Attentive Language Models beyond a Fixed-Length Context

## References

- Regularizing and Optimizing LSTM Language Models
- [Language Modeling with Gated Convolutional Networks](./language-modeling-with-gated-convolutional-networks.md)
- An Improved Relative Self-Attention Mechanism for Transformer with Application to Music Generation
- [Character-Level Language Modeling with Deeper Self-Attention](./character-level-language-modeling-with-deeper-self-attention.md)
- Learning Longer-term Dependencies in RNNs with Auxiliary Losses
- Sharp Nearby, Fuzzy Far Away - How Neural Language Models Use Context
- Recurrent Highway Networks
- Larger-Context Language Modelling
- An Analysis of Neural Language Modeling at Multiple Scales
- [Attention is All you Need](./attention-is-all-you-need.md)
- TopicRNN - A Recurrent Neural Network with Long-Range Semantic Dependency
- Multiplicative LSTM for sequence modelling
- Tying Word Vectors and Word Classifiers - A Loss Framework for Language Modeling
- Fast-Slow Recurrent Neural Networks
- [Exploring the Limits of Language Modeling](./exploring-the-limits-of-language-modeling.md)
- Breaking the Softmax Bottleneck - A High-Rank RNN Language Model
- One billion word benchmark for measuring progress in statistical language modeling
- [Improving Language Understanding by Generative Pre-Training](./improving-language-understanding-by-generative-pre-training.md)
- [Deep Contextualized Word Representations](./deep-contextualized-word-representations.md)
- [Pointer Sentinel Mixture Models](./pointer-sentinel-mixture-models.md)
- A Clockwork RNN
- Context dependent recurrent neural network language model
- [Self-Attention with Relative Position Representations](./self-attention-with-relative-position-representations.md)
- Fast Parametric Learning with Activation Memorization
- [Memory Networks](./memory-networks.md)
- [Long Short-Term Memory](./long-short-term-memory.md)
- Topic Compositional Neural Language Model
- Adaptive Input Representations for Neural Language Modeling
- Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation
- [Neural Machine Translation by Jointly Learning to Align and Translate](./neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
- [Neural Machine Translation in Linear Time](./neural-machine-translation-in-linear-time.md)
- Independently Recurrent Neural Network (IndRNN) - Building A Longer and Deeper RNN
- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](./an-empirical-evaluation-of-generic-convolutional-and-recurrent-networks-for-sequence-modeling.md)
- [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](./outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer.md)
- Neural Architecture Search with Reinforcement Learning
- [Using the Output Embedding to Improve Language Models](./using-the-output-embedding-to-improve-language-models.md)
- [Recurrent Batch Normalization](./recurrent-batch-normalization.md)
- Sparse Attentive Backtracking - Temporal CreditAssignment Through Reminding
- Semi-supervised Sequence Learning
- Learning Longer Memory in Recurrent Neural Networks
- Improving Neural Language Models with a Continuous Cache
- Document Context Language Models
- A Neural Probabilistic Language Model
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
- Hierarchical Probabilistic Neural Network Language Model
- Efficient Neural Architecture Search via Parameter Sharing
- [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](./a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
- Efficient softmax approximation for GPUs
- Factorization tricks for LSTM networks
- DARTS - Differentiable Architecture Search
- Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
- [On Multiplicative Integration with Recurrent Neural Networks](./on-multiplicative-integration-with-recurrent-neural-networks.md)
- HyperNetworks
- [Recurrent neural network based language model](./recurrent-neural-network-based-language-model.md)
- [Mesh-TensorFlow - Deep Learning for Supercomputers](./mesh-tensorflow-deep-learning-for-supercomputers.md)
- [Recurrent Neural Network Regularization](./recurrent-neural-network-regularization.md)
- [Generating Sequences With Recurrent Neural Networks](./generating-sequences-with-recurrent-neural-networks.md)
- [Hierarchical Multiscale Recurrent Neural Networks](./hierarchical-multiscale-recurrent-neural-networks.md)
- Pushing the bounds of dropout
- Sigsoftmax - Reanalysis of the Softmax Bottleneck
- Understanding the exploding gradient problem
- [Neural Turing Machines](./neural-turing-machines.md)
- 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
